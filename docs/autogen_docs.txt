class AssistantAgent(ConversableAgent) class AssistantAgent(ConversableAgent) (In preview) Assistant agent, designed to solve a task with LLM. AssistantAgent is a subclass of ConversableAgent configured with a default system message.
The default system message is designed to solve a task with LLM,
including suggesting python code blocks and debugging.
human_input_mode is default to "NEVER"
and code_execution_config is default to False.
This agent doesn't execute code by default, and expects the user to execute the code. human_input_mode code_execution_config def __init__(name: str,             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,             llm_config: Optional[Union[Dict, Literal[False]]] = None,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "NEVER",             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = False,             description: Optional[str] = None,             **kwargs) def __init__(name: str,             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,             llm_config: Optional[Union[Dict, Literal[False]]] = None,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "NEVER",             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = False,             description: Optional[str] = None,             **kwargs) Arguments: name system_message llm_config is_termination_msg max_consecutive_auto_reply **kwargsclass TextAnalyzerAgent(ConversableAgent) class TextAnalyzerAgent(ConversableAgent) (Experimental) Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed. def __init__(name="analyzer",             system_message: Optional[str] = system_message,             human_input_mode: Optional[str] = "NEVER",             llm_config: Optional[Union[Dict, bool]] = None,             **kwargs) def __init__(name="analyzer",             system_message: Optional[str] = system_message,             human_input_mode: Optional[str] = "NEVER",             llm_config: Optional[Union[Dict, bool]] = None,             **kwargs) Arguments: name system_message human_input_mode llm_config teach_config **kwargs def analyze_text(text_to_analyze, analysis_instructions) def analyze_text(text_to_analyze, analysis_instructions) Analyzes the given text as instructed, and returns the analysis.There are multiple ways to construct configurations for LLM inference in the oai utilities: oai get_config_list config_list_openai_aoai config_list_from_json config_list_from_models config_list_from_dotenv .env We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints. Make sure the "config_list" is included in the llm_config in the constructor of the LLM-based agent. For example, llm_config assistant = autogen.AssistantAgent(    name="assistant",    llm_config={"config_list": config_list}) assistant = autogen.AssistantAgent(    name="assistant",    llm_config={"config_list": config_list}) The llm_config is used in the create function for LLM inference.
When llm_config is not provided, the agent will rely on other openai settings such as openai.api_key or the environment variable OPENAI_API_KEY, which can also work when you'd like to use a single endpoint.
You can also explicitly specify that by: llm_config create llm_config openai.api_key OPENAI_API_KEY assistant = autogen.AssistantAgent(name="assistant", llm_config={"api_key": ...}) assistant = autogen.AssistantAgent(name="assistant", llm_config={"api_key": ...}) In version >=1, OpenAI renamed their api_base parameter to base_url. So for older versions, use api_base but for newer versions use base_url. api_base base_url api_base base_url Yes. Please check https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs for an example. You can set max_retries to handle rate limit error. And you can set timeout to handle timeout error. They can all be specified in llm_config for an agent, which will be used in the OpenAI client for LLM inference. They can be set differently for different clients if they are set in the config_list. max_retries timeout llm_config config_list max_retries timeout Please refer to the documentation for more info. When you call initiate_chat the conversation restarts by default. You can use send or initiate_chat(clear_history=False) to continue the conversation. initiate_chat send initiate_chat(clear_history=False) Each agent can be customized. You can use LLMs, tools, or humans behind each agent. If you use an LLM for an agent, use the one best suited for its role. There is no limit of the number of agents, but start from a small number like 2, 3. The more capable is the LLM and the fewer roles you need, the fewer agents you need. The default user proxy agent doesn't use LLM. If you'd like to use an LLM in UserProxyAgent, the use case could be to simulate user's behavior. The default assistant agent is instructed to use both coding and language skills. It doesn't have to do coding, depending on the tasks. And you can customize the system message. So if you want to use it for coding, use a model that's good at coding. If you are using a custom system message for the coding agent, please include something like:
If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line.
in the system message. This line is in the default system message of the AssistantAgent. If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. AssistantAgent If the # filename doesn't appear in the suggested code still, consider adding explicit instructions such as "save the code to disk" in the initial user message in initiate_chat.
The AssistantAgent doesn't save all the code by default, because there are cases in which one would just like to finish a task without saving the code. # filename initiate_chat AssistantAgent We strongly recommend using docker to execute code. There are two ways to use docker: docker use_docker code_execution_config You might want to override the default docker image used for code execution. To do that set use_docker key of code_execution_config property to the name of the image. E.g.: use_docker code_execution_config user_proxy = autogen.UserProxyAgent(    name="agent",    human_input_mode="TERMINATE",    max_consecutive_auto_reply=10,    code_execution_config={"work_dir":"_output", "use_docker":"python:3"},    llm_config=llm_config,    system_message=""""Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.""") user_proxy = autogen.UserProxyAgent(    name="agent",    human_input_mode="TERMINATE",    max_consecutive_auto_reply=10,    code_execution_config={"work_dir":"_output", "use_docker":"python:3"},    llm_config=llm_config,    system_message=""""Reply TERMINATE if the task has been solved at full satisfaction.Otherwise, reply CONTINUE, or the reason why the task is not solved yet.""") If you have problems with agents running pip install or get errors similar to Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'), you can choose 'python:3' as image as shown in the code example above and that should solve the problem. pip install Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory') gpt-3.5-turbo When using gpt-3.5-turbo you may often encounter agents going into a "gratitude loop", meaning when they complete a task they will begin congratulating and thanking eachother in a continuous loop. This is a limitation in the performance of gpt-3.5-turbo, in contrast to gpt-4 which has no problem remembering instructions. This can hinder the experimentation experience when trying to test out your own use case with cheaper models. gpt-3.5-turbo gpt-3.5-turbo gpt-4 A workaround is to add an additional termination notice to the prompt. This acts a "little nudge" for the LLM to remember that they need to terminate the conversation when their task is complete. You can do this by appending a string such as the following to your user input string: prompt = "Some user query"termination_notice = (    '\n\nDo not show appreciation in your responses, say only what is necessary. '    'if "Thank you" or "You\'re welcome" are said in the conversation, then say TERMINATE '    'to indicate the conversation is finished and this is your last message.')prompt += termination_notice prompt = "Some user query"termination_notice = (    '\n\nDo not show appreciation in your responses, say only what is necessary. '    'if "Thank you" or "You\'re welcome" are said in the conversation, then say TERMINATE '    'to indicate the conversation is finished and this is your last message.')prompt += termination_notice Note: This workaround gets the job done around 90% of the time, but there are occurrences where the LLM still forgets to terminate the conversation. (from issue #251) Code examples that use chromadb (like retrieval) fail in codespaces due to a sqlite3 requirement. >>> import chromadbTraceback (most recent call last):  File "<stdin>", line 1, in <module>  File "/home/vscode/.local/lib/python3.10/site-packages/chromadb/__init__.py", line 69, in <module>    raise RuntimeError(RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade. >>> import chromadbTraceback (most recent call last):  File "<stdin>", line 1, in <module>  File "/home/vscode/.local/lib/python3.10/site-packages/chromadb/__init__.py", line 69, in <module>    raise RuntimeError(RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade. Workaround: pip install pysqlite3-binary mkdir /home/vscode/.local/lib/python3.10/site-packages/google/colab Explanation: Per this gist, linked from the official chromadb docs, adding this folder triggers chromadb to use pysqlite3 instead of the default. (from issue #478) See here https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#register_reply  For example, you can register a reply function that gets called when generate_reply is called for an agent. generate_reply def print_messages(recipient, messages, sender, config):    if "callback" in config and  config["callback"] is not None:        callback = config["callback"]        callback(sender, recipient, messages[-1])    print(f"Messages sent to: {recipient.name} | num messages: {len(messages)}")    return False, None  # required to ensure the agent communication flow continuesuser_proxy.register_reply(    [autogen.Agent, None],    reply_func=print_messages,    config={"callback": None},)assistant.register_reply(    [autogen.Agent, None],    reply_func=print_messages,    config={"callback": None},) def print_messages(recipient, messages, sender, config):    if "callback" in config and  config["callback"] is not None:        callback = config["callback"]        callback(sender, recipient, messages[-1])    print(f"Messages sent to: {recipient.name} | num messages: {len(messages)}")    return False, None  # required to ensure the agent communication flow continuesuser_proxy.register_reply(    [autogen.Agent, None],    reply_func=print_messages,    config={"callback": None},)assistant.register_reply(    [autogen.Agent, None],    reply_func=print_messages,    config={"callback": None},) In the above, we register a print_messages function that is called each time the agent's generate_reply is triggered after receiving a message. print_messages generate_reply Refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#last_message Please refer to https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#chat_messages gpt-3.5-turboclass GPTAssistantAgent(ConversableAgent) class GPTAssistantAgent(ConversableAgent) An experimental AutoGen agent class that leverages the OpenAI Assistant API for conversational capabilities.
This agent is unique in its reliance on the OpenAI Assistant for state management, differing from other agents like ConversableAgent. def __init__(name="GPT Assistant",             instructions: Optional[str] = None,             llm_config: Optional[Union[Dict, bool]] = None,             overwrite_instructions: bool = False,             **kwargs) def __init__(name="GPT Assistant",             instructions: Optional[str] = None,             llm_config: Optional[Union[Dict, bool]] = None,             overwrite_instructions: bool = False,             **kwargs) Arguments: name instructions llm_config overwrite_instructions kwargs def can_execute_function(name: str) -> bool def can_execute_function(name: str) -> bool Whether the agent can execute the function. def reset() def reset() Resets the agent, clearing any existing conversation thread and unread message indices. def clear_history(agent: Optional[Agent] = None) def clear_history(agent: Optional[Agent] = None) Clear the chat history of the agent. Arguments: agent def pretty_print_thread(thread) def pretty_print_thread(thread) Pretty print the thread. @propertydef oai_threads() -> Dict[Agent, Any] @propertydef oai_threads() -> Dict[Agent, Any] Return the threads of the agent. @propertydef assistant_id() @propertydef assistant_id() Return the assistant id def get_assistant_instructions() def get_assistant_instructions() Return the assistant instructions from OAI assistant API def delete_assistant() def delete_assistant() Delete the assistant from OAI assistant APIAutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation via multi-agent conversation.
Please find documentation about this feature here. Links to notebook examples: Code Generation, Execution, and Debugging Multi-Agent Collaboration (>3 Agents) Applications Tool Use Human Involvement Agent Teaching and Learning Multi-Agent Chat with OpenAI Assistants in the loop Multimodal Agent Long Context Handling Evaluation and Assessment Automatic Agent Building AutoGen offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. The research study finds that tuning hyperparameters can significantly improve the utility of them.
Please find documentation about this feature here. Links to notebook examples:def get_key(config) def get_key(config) Get a unique identifier of a configuration. Arguments: config Returns: tuple def get_config_list(api_keys: List,                    base_urls: Optional[List] = None,                    api_type: Optional[str] = None,                    api_version: Optional[str] = None) -> List[Dict] def get_config_list(api_keys: List,                    base_urls: Optional[List] = None,                    api_type: Optional[str] = None,                    api_version: Optional[str] = None) -> List[Dict] Get a list of configs for openai api calls. Arguments: api_keys base_urls api_type api_version def config_list_openai_aoai(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None) -> List[Dict] def config_list_openai_aoai(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None) -> List[Dict] Get a list of configs for openai + azure openai api calls. Arguments: key_file_path openai_api_key_file aoai_api_key_file aoai_api_base_file exclude Returns: list def config_list_from_models(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None,        model_list: Optional[list] = None) -> List[Dict] def config_list_from_models(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None,        model_list: Optional[list] = None) -> List[Dict] Get a list of configs for api calls with models in the model list. Arguments: key_file_path openai_api_key_file aoai_api_key_file aoai_api_base_file exclude model_list Returns: list def config_list_gpt4_gpt35(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None) -> List[Dict] def config_list_gpt4_gpt35(        key_file_path: Optional[str] = ".",        openai_api_key_file: Optional[str] = "key_openai.txt",        aoai_api_key_file: Optional[str] = "key_aoai.txt",        aoai_api_base_file: Optional[str] = "base_aoai.txt",        exclude: Optional[str] = None) -> List[Dict] Get a list of configs for gpt-4 followed by gpt-3.5 api calls. Arguments: key_file_path openai_api_key_file aoai_api_key_file aoai_api_base_file exclude Returns: list def filter_config(config_list, filter_dict) def filter_config(config_list, filter_dict) Filter the config list by provider and model. Arguments: config_list filter_dict Returns: list def config_list_from_json(    env_or_file: str,    file_location: Optional[str] = "",    filter_dict: Optional[Dict[str, Union[List[Union[str, None]],                                          Set[Union[str, None]]]]] = None) -> List[Dict] def config_list_from_json(    env_or_file: str,    file_location: Optional[str] = "",    filter_dict: Optional[Dict[str, Union[List[Union[str, None]],                                          Set[Union[str, None]]]]] = None) -> List[Dict] Get a list of configs from a json parsed from an env variable or a file. Arguments: env_or_file file_location filter_dict filter_dict = {    "api_type": ["open_ai", None],  # None means a missing key is acceptable    "model": ["gpt-3.5-turbo", "gpt-4"],} filter_dict = {    "api_type": ["open_ai", None],  # None means a missing key is acceptable    "model": ["gpt-3.5-turbo", "gpt-4"],} Returns: list def get_config(api_key: str,               base_url: Optional[str] = None,               api_type: Optional[str] = None,               api_version: Optional[str] = None) -> Dict def get_config(api_key: str,               base_url: Optional[str] = None,               api_type: Optional[str] = None,               api_version: Optional[str] = None) -> Dict Construct a configuration dictionary with the provided API configurations.
Appending the additional configurations to the config only if they're set example: model_api_key_map={
"gpt-4": "OPENAI_API_KEY",
"gpt-3.5-turbo": {
"api_key_env_var": "ANOTHER_API_KEY",
"api_type": "aoai",
"api_version": "v2",
"base_url": "https://api.someotherapi.com"
}
} Arguments: api_key base_url api_type api_version Returns: Dict def config_list_from_dotenv(    dotenv_file_path: Optional[str] = None,    model_api_key_map: Optional[dict] = None,    filter_dict: Optional[dict] = None) -> List[Dict[str, Union[str, Set[str]]]] def config_list_from_dotenv(    dotenv_file_path: Optional[str] = None,    model_api_key_map: Optional[dict] = None,    filter_dict: Optional[dict] = None) -> List[Dict[str, Union[str, Set[str]]]] Load API configurations from a specified .env file or environment variables and construct a list of configurations. This function will: model_api_key_map will default to {"gpt-4": "OPENAI_API_KEY", "gpt-3.5-turbo": "OPENAI_API_KEY"} if none {"gpt-4": "OPENAI_API_KEY", "gpt-3.5-turbo": "OPENAI_API_KEY"} Arguments: dotenv_file_path model_api_key_map filter_dict Returns:   List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model. Raises: FileNotFoundError TypeError def retrieve_assistants_by_name(client, name) -> str def retrieve_assistants_by_name(client, name) -> str Return the assistants with the given name from OAI assistant APIAutoGen offers a unified multi-agent conversation framework as a high-level abstraction of using foundation models. It features capable, customizable and conversable agents which integrate LLMs, tools, and humans via automated agent chat.
By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. This framework simplifies the orchestration, automation and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcome their weaknesses. It enables building next-gen LLM applications based on multi-agent conversations with minimal effort. AutoGen abstracts and implements conversable agents
designed to solve tasks through inter-agent conversations. Specifically, the agents in AutoGen have the following notable features: Conversable: Agents in AutoGen are conversable, which means that any agent can send
and receive messages from other agents to initiate or continue a conversation Customizable: Agents in AutoGen can be customized to integrate LLMs, humans, tools, or a combination of them. The figure below shows the built-in agents in AutoGen.
 We have designed a generic ConversableAgent
class for Agents that are capable of conversing with each other through the exchange of messages to jointly finish a task. An agent can communicate with other agents and perform actions. Different agents can differ in what actions they perform after receiving messages. Two representative subclasses are AssistantAgent and UserProxyAgent ConversableAgent AssistantAgent UserProxyAgent The AssistantAgent is designed to act as an AI assistant, using LLMs by default but not requiring human input or code execution. It could write Python code (in a Python coding block) for a user to execute when a message (typically a description of a task that needs to be solved) is received. Under the hood, the Python code is written by LLM (e.g., GPT-4). It can also receive the execution results and suggest corrections or bug fixes. Its behavior can be altered by passing a new system message. The LLM inference configuration can be configured via [llm_config]. AssistantAgent llm_config The UserProxyAgent is conceptually a proxy agent for humans, soliciting human input as the agent's reply at each interaction turn by default and also having the capability to execute code and call functions. The UserProxyAgent triggers code execution automatically when it detects an executable code block in the received message and no human user input is provided. Code execution can be disabled by setting the code_execution_config parameter to False. LLM-based response is disabled by default. It can be enabled by setting llm_config to a dict corresponding to the inference configuration. When llm_config is set as a dictionary, UserProxyAgent can generate replies using an LLM when code execution is not performed. UserProxyAgent UserProxyAgent code_execution_config llm_config llm_config UserProxyAgent The auto-reply capability of ConversableAgent allows for more autonomous multi-agent communication while retaining the possibility of human intervention.
One can also easily extend it by registering reply functions with the register_reply() method. ConversableAgent register_reply() In the following code, we create an AssistantAgent  named "assistant" to serve as the assistant and a UserProxyAgent named "user_proxy" to serve as a proxy for the human user. We will later employ these two agents to solve a task. AssistantAgent UserProxyAgent from autogen import AssistantAgent, UserProxyAgent# create an AssistantAgent instance named "assistant"assistant = AssistantAgent(name="assistant")# create a UserProxyAgent instance named "user_proxy"user_proxy = UserProxyAgent(name="user_proxy") from autogen import AssistantAgent, UserProxyAgent# create an AssistantAgent instance named "assistant"assistant = AssistantAgent(name="assistant")# create a UserProxyAgent instance named "user_proxy"user_proxy = UserProxyAgent(name="user_proxy") Once the participating agents are constructed properly, one can start a multi-agent conversation session by an initialization step as shown in the following code: # the assistant receives a message from the user, which contains the task descriptionuser_proxy.initiate_chat(    assistant,    message="""What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?""",) # the assistant receives a message from the user, which contains the task descriptionuser_proxy.initiate_chat(    assistant,    message="""What date is today? Which big tech stock has the largest year-to-date gain this year? How much is the gain?""",) After the initialization step, the conversation could proceed automatically. Find a visual illustration of how the user_proxy and assistant collaboratively solve the above task autonomously below:
 On the one hand, one can achieve fully autonomous conversations after an initialization step. On the other hand, AutoGen can be used to implement human-in-the-loop problem-solving by configuring human involvement levels and patterns (e.g., setting the human_input_mode to ALWAYS), as human involvement is expected and/or desired in many applications. human_input_mode ALWAYS By adopting the conversation-driven control with both programming language and natural language, AutoGen inherently allows dynamic conversation. Dynamic conversation allows the agent topology to change depending on the actual flow of conversation under different input problem instances, while the flow of a static conversation always follows a pre-defined topology. The dynamic conversation pattern is useful in complex applications where the patterns of interaction cannot be predetermined in advance. AutoGen provides two general approaches to achieving dynamic conversation: Registered auto-reply. With the pluggable auto-reply function, one can choose to invoke conversations with other agents depending on the content of the current message and context. A working system demonstrating this type of dynamic conversation can be found in this code example, demonstrating a dynamic group chat. In the system, we register an auto-reply function in the group chat manager, which lets LLM decide who the next speaker will be in a group chat setting. LLM-based function call. In this approach, LLM decides whether or not to call a particular function depending on the conversation status in each inference call.
By messaging additional agents in the called functions, the LLM can drive dynamic multi-agent conversation. A working system showcasing this type of dynamic conversation can be found in the multi-user math problem solving scenario, where a student assistant would automatically resort to an expert using function calls. The figure below shows six examples of applications built using AutoGen.
 Find a list of examples in this page: Automated Agent Chat Examples Interested in the research that leads to this package? Please check the following papers. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang and Chi Wang. ArXiv 2023. An Empirical Study on Challenging Math Problem Solving with GPT-4. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang. ArXiv preprint arXiv:2306.01337 (2023).For technical details, please check our technical report and research publications. @inproceedings{wu2023autogen,      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},      year={2023},      eprint={2308.08155},      archivePrefix={arXiv},      primaryClass={cs.AI}} @inproceedings{wu2023autogen,      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},      year={2023},      eprint={2308.08155},      archivePrefix={arXiv},      primaryClass={cs.AI}} @inproceedings{wang2023EcoOptiGen,    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},    year={2023},    booktitle={AutoML'23},} @inproceedings{wang2023EcoOptiGen,    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},    year={2023},    booktitle={AutoML'23},} @inproceedings{wu2023empirical,    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},    year={2023},    booktitle={ArXiv preprint arXiv:2306.01337},} @inproceedings{wu2023empirical,    title={An Empirical Study on Challenging Math Problem Solving with GPT-4},    author={Yiran Wu and Feiran Jia and Shaokun Zhang and Hangyu Li and Erkang Zhu and Yue Wang and Yin Tat Lee and Richard Peng and Qingyun Wu and Chi Wang},    year={2023},    booktitle={ArXiv preprint arXiv:2306.01337},} @inproceedings{zhang2023ecoassistant,    title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},    author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},    year={2023},    booktitle={ArXiv preprint arXiv:2310.03046},} @inproceedings{zhang2023ecoassistant,    title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},    author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},    year={2023},    booktitle={ArXiv preprint arXiv:2310.03046},}AutoGen is a framework that enables development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.  AutoGen is powered by collaborative research studies from Microsoft, Penn State University, and University of Washington. Install from pip: pip install pyautogen. Find more options in Installation.
For code execution, we strongly recommend installing the python docker package, and using docker. pip install pyautogen Autogen enables the next-gen LLM applications with a generic multi-agent conversation framework. It offers customizable and conversable agents which integrate LLMs, tools, and humans.
By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code. For example, from autogen import AssistantAgent, UserProxyAgent, config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant = AssistantAgent("assistant", llm_config={"config_list": config_list})user_proxy = UserProxyAgent("user_proxy", code_execution_config={"work_dir": "coding"})user_proxy.initiate_chat(assistant, message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the task from autogen import AssistantAgent, UserProxyAgent, config_list_from_json# Load LLM inference endpoints from an env variable or a file# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints# and OAI_CONFIG_LIST_sample.jsonconfig_list = config_list_from_json(env_or_file="OAI_CONFIG_LIST")assistant = AssistantAgent("assistant", llm_config={"config_list": config_list})user_proxy = UserProxyAgent("user_proxy", code_execution_config={"work_dir": "coding"})user_proxy.initiate_chat(assistant, message="Plot a chart of NVDA and TESLA stock price change YTD.")# This initiates an automated chat between the two agents to solve the task The figure below shows an example conversation flow with AutoGen.
 Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers enhanced LLM inference with powerful functionalities like tuning, caching, error handling, templating. For example, you can optimize generations by LLM with your own tuning data, success metrics and budgets. # perform tuning for openai<1config, analysis = autogen.Completion.tune(    data=tune_data,    metric="success",    mode="max",    eval_func=eval_func,    inference_budget=0.05,    optimization_budget=3,    num_samples=-1,)# perform inference for a test instanceresponse = autogen.Completion.create(context=test_instance, **config) # perform tuning for openai<1config, analysis = autogen.Completion.tune(    data=tune_data,    metric="success",    mode="max",    eval_func=eval_func,    inference_budget=0.05,    optimization_budget=3,    num_samples=-1,)# perform inference for a test instanceresponse = autogen.Completion.create(context=test_instance, **config) If you like our project, please give it a star on GitHub. If you are interested in contributing, please read Contributor's Guide.class Completion(openai_Completion) class Completion(openai_Completion) (openai<1) A class for OpenAI completion API. It also supports: ChatCompletion, Azure OpenAI API. @classmethoddef set_cache(cls,              seed: Optional[int] = 41,              cache_path_root: Optional[str] = ".cache") @classmethoddef set_cache(cls,              seed: Optional[int] = 41,              cache_path_root: Optional[str] = ".cache") Set cache path. Arguments: seed cache_path @classmethoddef clear_cache(cls,                seed: Optional[int] = None,                cache_path_root: Optional[str] = ".cache") @classmethoddef clear_cache(cls,                seed: Optional[int] = None,                cache_path_root: Optional[str] = ".cache") Clear cache. Arguments: seed cache_path @classmethoddef tune(cls,         data: List[Dict],         metric: str,         mode: str,         eval_func: Callable,         log_file_name: Optional[str] = None,         inference_budget: Optional[float] = None,         optimization_budget: Optional[float] = None,         num_samples: Optional[int] = 1,         logging_level: Optional[int] = logging.WARNING,         **config) @classmethoddef tune(cls,         data: List[Dict],         metric: str,         mode: str,         eval_func: Callable,         log_file_name: Optional[str] = None,         inference_budget: Optional[float] = None,         optimization_budget: Optional[float] = None,         num_samples: Optional[int] = 1,         logging_level: Optional[int] = logging.WARNING,         **config) Tune the parameters for the OpenAI API call. TODO: support parallel tuning with ray or spark.
TODO: support agg_method as in test Arguments: data metric mode eval_func def eval_func(responses, **data):    solution = data["solution"]    success_list = []    n = len(responses)    for i in range(n):        response = responses[i]        succeed = is_equiv_chain_of_thought(response, solution)        success_list.append(succeed)    return {        "expected_success": 1 - pow(1 - sum(success_list) / n, n),        "success": any(s for s in success_list),    } def eval_func(responses, **data):    solution = data["solution"]    success_list = []    n = len(responses)    for i in range(n):        response = responses[i]        succeed = is_equiv_chain_of_thought(response, solution)        success_list.append(succeed)    return {        "expected_success": 1 - pow(1 - sum(success_list) / n, n),        "success": any(s for s in success_list),    } log_file_name inference_budget optimization_budget num_samples logging_level **config prompt.format(**data) prompt(data) Returns: dict tune.ExperimentAnalysis @classmethoddef create(cls,           context: Optional[Dict] = None,           use_cache: Optional[bool] = True,           config_list: Optional[List[Dict]] = None,           filter_func: Optional[Callable[[Dict, Dict], bool]] = None,           raise_on_ratelimit_or_timeout: Optional[bool] = True,           allow_format_str_template: Optional[bool] = False,           **config) @classmethoddef create(cls,           context: Optional[Dict] = None,           use_cache: Optional[bool] = True,           config_list: Optional[List[Dict]] = None,           filter_func: Optional[Callable[[Dict, Dict], bool]] = None,           raise_on_ratelimit_or_timeout: Optional[bool] = True,           allow_format_str_template: Optional[bool] = False,           **config) Make a completion for a given context. Arguments: context prompt="Complete the following sentence: {prefix}, context={"prefix": "Today I feel"} use_cache config_list response = oai.Completion.create(    config_list=[        {            "model": "gpt-4",            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),            "api_type": "azure",            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),            "api_version": "2023-03-15-preview",        },        {            "model": "gpt-3.5-turbo",            "api_key": os.environ.get("OPENAI_API_KEY"),            "api_type": "open_ai",            "base_url": "https://api.openai.com/v1",        },        {            "model": "llama-7B",            "base_url": "http://127.0.0.1:8080",            "api_type": "open_ai",        }    ],    prompt="Hi",) response = oai.Completion.create(    config_list=[        {            "model": "gpt-4",            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),            "api_type": "azure",            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),            "api_version": "2023-03-15-preview",        },        {            "model": "gpt-3.5-turbo",            "api_key": os.environ.get("OPENAI_API_KEY"),            "api_type": "open_ai",            "base_url": "https://api.openai.com/v1",        },        {            "model": "llama-7B",            "base_url": "http://127.0.0.1:8080",            "api_type": "open_ai",        }    ],    prompt="Hi",) filter_func def yes_or_no_filter(context, config, response):    return context.get("yes_or_no_choice", False) is False or any(        text in ["Yes.", "No."] for text in oai.Completion.extract_text(response)    ) def yes_or_no_filter(context, config, response):    return context.get("yes_or_no_choice", False) is False or any(        text in ["Yes.", "No."] for text in oai.Completion.extract_text(response)    ) raise_on_ratelimit_or_timeout allow_format_str_template **config max_retry_period retry_wait_time cache_seed Returns:   Responses from OpenAI API, with additional fields. cost config_list config_id pass_filter @classmethoddef test(cls,         data,         eval_func=None,         use_cache=True,         agg_method="avg",         return_responses_and_per_instance_result=False,         logging_level=logging.WARNING,         **config) @classmethoddef test(cls,         data,         eval_func=None,         use_cache=True,         agg_method="avg",         return_responses_and_per_instance_result=False,         logging_level=logging.WARNING,         **config) Evaluate the responses created with the config for the OpenAI API call. Arguments: data eval_func def eval_func(responses, **data):    solution = data["solution"]    success_list = []    n = len(responses)    for i in range(n):        response = responses[i]        succeed = is_equiv_chain_of_thought(response, solution)        success_list.append(succeed)    return {        "expected_success": 1 - pow(1 - sum(success_list) / n, n),        "success": any(s for s in success_list),    } def eval_func(responses, **data):    solution = data["solution"]    success_list = []    n = len(responses)    for i in range(n):        response = responses[i]        succeed = is_equiv_chain_of_thought(response, solution)        success_list.append(succeed)    return {        "expected_success": 1 - pow(1 - sum(success_list) / n, n),        "success": any(s for s in success_list),    } use_cache agg_method agg_method = 'median' agg_method = 'median'   An example agg_method in a Callable: agg_method = np.median agg_method = np.median   An example agg_method in a dict of Callable: agg_method={'median_success': np.median, 'avg_success': np.mean} agg_method={'median_success': np.median, 'avg_success': np.mean} return_responses_and_per_instance_result logging_level **config create() Returns:   None when no valid eval_func is provided in either test or tune;
Otherwise, a dict of aggregated results, responses and per instance results if return_responses_and_per_instance_result is True;
Otherwise, a dict of aggregated results (responses and per instance results are not returned). return_responses_and_per_instance_result @classmethoddef cost(cls, response: dict) @classmethoddef cost(cls, response: dict) Compute the cost of an API call. Arguments: response Returns:   The cost in USD. 0 if the model is not supported. @classmethoddef extract_text(cls, response: dict) -> List[str] @classmethoddef extract_text(cls, response: dict) -> List[str] Extract the text from a completion or chat response. Arguments: response Returns:   A list of text in the responses. @classmethoddef extract_text_or_function_call(cls, response: dict) -> List[str] @classmethoddef extract_text_or_function_call(cls, response: dict) -> List[str] Extract the text or function calls from a completion or chat response. Arguments: response Returns:   A list of text or function calls in the responses. @classmethod@propertydef logged_history(cls) -> Dict @classmethod@propertydef logged_history(cls) -> Dict Return the book keeping dictionary. @classmethoddef print_usage_summary(cls) -> Dict @classmethoddef print_usage_summary(cls) -> Dict Return the usage summary. @classmethoddef start_logging(cls,                  history_dict: Optional[Dict] = None,                  compact: Optional[bool] = True,                  reset_counter: Optional[bool] = True) @classmethoddef start_logging(cls,                  history_dict: Optional[Dict] = None,                  compact: Optional[bool] = True,                  reset_counter: Optional[bool] = True) Start book keeping. Arguments: history_dict compact {    "create_at": [0, 1],    "cost": [0.1, 0.2],} {    "create_at": [0, 1],    "cost": [0.1, 0.2],}   where "created_at" is the index of API calls indicating the order of all the calls,
and "cost" is the cost of each call. This example shows that the conversation is based
on two API calls. The compact format is useful for condensing the history of a conversation.
If compact is False, the history dictionary will contain all the API calls: the key
is the index of the API call, and the value is a dictionary like: {    "request": request_dict,    "response": response_dict,} {    "request": request_dict,    "response": response_dict,}   where request_dict is the request sent to OpenAI API, and response_dict is the response.
For a conversation containing two API calls, the non-compact history dictionary will be like: {    0: {        "request": request_dict_0,        "response": response_dict_0,    },    1: {        "request": request_dict_1,        "response": response_dict_1,    }, {    0: {        "request": request_dict_0,        "response": response_dict_0,    },    1: {        "request": request_dict_1,        "response": response_dict_1,    },   The first request's messages plus the response is equal to the second request's messages.
For a conversation with many turns, the non-compact history dictionary has a quadratic size
while the compact history dict has a linear size. reset_counter @classmethoddef stop_logging(cls) @classmethoddef stop_logging(cls) End book keeping. class ChatCompletion(Completion) class ChatCompletion(Completion) (openai<1) A class for OpenAI API ChatCompletion. Share the same API as Completion.class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent) class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent) def __init__(name="RetrieveChatAgent",             human_input_mode: Optional[str] = "ALWAYS",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             retrieve_config: Optional[Dict] = None,             **kwargs) def __init__(name="RetrieveChatAgent",             human_input_mode: Optional[str] = "ALWAYS",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             retrieve_config: Optional[Dict] = None,             **kwargs) Arguments: name human_input_mode is_termination_msg retrieve_config default retrieve_docs autogen-docs gpt-4 max_tokens * 0.4 max_tokens * 0.8 multi_lines BAAI/bge-small-en-v1.5 https://qdrant.github.io/fastembed/examples/Supported_Models/ Update Context Update Context autogen.retrieve_utils.split_text_to_chunks autogen.retrieve_utils.TEXT_FORMATS docs_path **kwargs def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "") def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "") Arguments: problem n_results search_string def create_qdrant_from_dir(        dir_path: str,        max_tokens: int = 4000,        client: QdrantClient = None,        collection_name: str = "all-my-documents",        chunk_mode: str = "multi_lines",        must_break_at_empty_line: bool = True,        embedding_model: str = "BAAI/bge-small-en-v1.5",        custom_text_split_function: Callable = None,        custom_text_types: List[str] = TEXT_FORMATS,        recursive: bool = True,        parallel: int = 0,        on_disk: bool = False,        quantization_config: Optional[models.QuantizationConfig] = None,        hnsw_config: Optional[models.HnswConfigDiff] = None,        payload_indexing: bool = False,        qdrant_client_options: Optional[Dict] = {}) def create_qdrant_from_dir(        dir_path: str,        max_tokens: int = 4000,        client: QdrantClient = None,        collection_name: str = "all-my-documents",        chunk_mode: str = "multi_lines",        must_break_at_empty_line: bool = True,        embedding_model: str = "BAAI/bge-small-en-v1.5",        custom_text_split_function: Callable = None,        custom_text_types: List[str] = TEXT_FORMATS,        recursive: bool = True,        parallel: int = 0,        on_disk: bool = False,        quantization_config: Optional[models.QuantizationConfig] = None,        hnsw_config: Optional[models.HnswConfigDiff] = None,        payload_indexing: bool = False,        qdrant_client_options: Optional[Dict] = {}) Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a
url to a single file. Arguments: dir_path max_tokens client collection_name chunk_mode must_break_at_empty_line embedding_model custom_text_split_function autogen.retrieve_utils.split_text_to_chunks custom_text_types recursive parallel on_disk quantization_config Ref hnsw_config Ref payload_indexing qdrant_client_options Ref def query_qdrant(        query_texts: List[str],        n_results: int = 10,        client: QdrantClient = None,        collection_name: str = "all-my-documents",        search_string: str = "",        embedding_model: str = "BAAI/bge-small-en-v1.5",        qdrant_client_options: Optional[Dict] = {}) -> List[List[QueryResponse]] def query_qdrant(        query_texts: List[str],        n_results: int = 10,        client: QdrantClient = None,        collection_name: str = "all-my-documents",        search_string: str = "",        embedding_model: str = "BAAI/bge-small-en-v1.5",        qdrant_client_options: Optional[Dict] = {}) -> List[List[QueryResponse]] Perform a similarity search with filters on a Qdrant collection Arguments: query_texts n_results client collection_name search_string embedding_model qdrant_client_options Returns: List[List[QueryResponse]] id embedding metadata document score@dataclassclass GroupChat() @dataclassclass GroupChat() (In preview) A group chat class that contains the following data fields: function_map agents @propertydef agent_names() -> List[str] @propertydef agent_names() -> List[str] Return the names of the agents in the group chat. def reset() def reset() Reset the group chat. def append(message: Dict) def append(message: Dict) Append a message to the group chat.
We cast the content to str here so that it can be managed by text-based
model. def agent_by_name(name: str) -> Agent def agent_by_name(name: str) -> Agent Returns the agent with a given name. def next_agent(agent: Agent, agents: Optional[List[Agent]] = None) -> Agent def next_agent(agent: Agent, agents: Optional[List[Agent]] = None) -> Agent Return the next agent in the list. def select_speaker_msg(agents: Optional[List[Agent]] = None) -> str def select_speaker_msg(agents: Optional[List[Agent]] = None) -> str Return the system message for selecting the next speaker. This is always the first message in the context. def select_speaker_prompt(agents: Optional[List[Agent]] = None) -> str def select_speaker_prompt(agents: Optional[List[Agent]] = None) -> str Return the floating system prompt selecting the next speaker. This is always the last message in the context. def manual_select_speaker(        agents: Optional[List[Agent]] = None) -> Union[Agent, None] def manual_select_speaker(        agents: Optional[List[Agent]] = None) -> Union[Agent, None] Manually select the next speaker. def select_speaker(last_speaker: Agent, selector: ConversableAgent) def select_speaker(last_speaker: Agent, selector: ConversableAgent) Select the next speaker. async def a_select_speaker(last_speaker: Agent, selector: ConversableAgent) async def a_select_speaker(last_speaker: Agent, selector: ConversableAgent) Select the next speaker. class GroupChatManager(ConversableAgent) class GroupChatManager(ConversableAgent) (In preview) A chat manager agent that can manage a group chat of multiple agents. def run_chat(messages: Optional[List[Dict]] = None,             sender: Optional[Agent] = None,             config: Optional[GroupChat] = None) -> Union[str, Dict, None] def run_chat(messages: Optional[List[Dict]] = None,             sender: Optional[Agent] = None,             config: Optional[GroupChat] = None) -> Union[str, Dict, None] Run a group chat. async def a_run_chat(messages: Optional[List[Dict]] = None,                     sender: Optional[Agent] = None,                     config: Optional[GroupChat] = None) async def a_run_chat(messages: Optional[List[Dict]] = None,                     sender: Optional[Agent] = None,                     config: Optional[GroupChat] = None) Run a group chat asynchronously.def llava_formatter(prompt: str,                    order_image_tokens: bool = False) -> Tuple[str, List[str]] def llava_formatter(prompt: str,                    order_image_tokens: bool = False) -> Tuple[str, List[str]] Formats the input prompt by replacing image tags and returns the new prompt along with image locations. Arguments: Returns: def gpt4v_formatter(prompt: str) -> List[Union[str, dict]] def gpt4v_formatter(prompt: str) -> List[Union[str, dict]] Formats the input prompt by replacing image tags and returns a list of text and images. Arguments: Returns: def extract_img_paths(paragraph: str) -> list def extract_img_paths(paragraph: str) -> list Extract image paths (URLs or local paths) from a text paragraph. Arguments: paragraph Returns: listWhen not using a docker container, we recommend using a virtual environment to install AutoGen. This will ensure that the dependencies for AutoGen are isolated from the rest of your system. You can create a virtual environment with venv as below: venv python3 -m venv pyautogensource pyautogen/bin/activate python3 -m venv pyautogensource pyautogen/bin/activate The following command will deactivate the current venv environment: venv deactivate deactivate Another option is with Conda, Conda works better at solving dependency conflicts than pip. You can install it by following this doc,
and then create a virtual environment as below: Conda conda create -n pyautogen python=3.10  # python 3.10 is recommended as it's stable and not too oldconda activate pyautogen conda create -n pyautogen python=3.10  # python 3.10 is recommended as it's stable and not too oldconda activate pyautogen The following command will deactivate the current conda environment: conda conda deactivate conda deactivate Now, you're ready to install AutoGen in the virtual environment you've just created. AutoGen requires Python version >= 3.8, < 3.12. It can be installed from pip: pip install pyautogen pip install pyautogen pyautogen<0.2 requires openai<1. Starting from pyautogen v0.2, openai>=1 is required. pyautogen<0.2 openai<1 openai>=1 openai v1 is a total rewrite of the library with many breaking changes. For example, the inference requires instantiating a client, instead of using a global class method.
Therefore, some changes are required for users of pyautogen<0.2. pyautogen<0.2 api_base base_url request_timeout timeout llm_config config_list max_retry_period retry_wait_time max_retries autogen.Completion autogen.ChatCompletion autogen.OpenAIWrapper from autogen import OpenAIWrapperclient = OpenAIWrapper(config_list=config_list)response = client.create(messages=[{"role": "user", "content": "2+2="}])print(client.extract_text_or_completion_object(response)) from autogen import OpenAIWrapperclient = OpenAIWrapper(config_list=config_list)response = client.create(messages=[{"role": "user", "content": "2+2="}])print(client.extract_text_or_completion_object(response)) OpenAIWrapper flaml.tune seed cache_seed seed use_cache OpenAIWrapper.create() cache_seed cache_seed seed seed seed cache_seed For the best user experience and seamless code execution, we highly recommend using Docker with AutoGen. Docker is a containerization platform that simplifies the setup and execution of your code. Developing in a docker container, such as GitHub Codespace, also makes the development convenient. When running AutoGen out of a docker container, to use docker for code execution, you also need to install the python package docker: docker pip install docker pip install docker pyautogen<0.2 offers a cost-effective hyperparameter optimization technique EcoOptiGen for tuning Large Language Models. Please install with the [blendsearch] option to use it. pyautogen<0.2 pip install "pyautogen[blendsearch]<0.2" pip install "pyautogen[blendsearch]<0.2" Example notebooks: Optimize for Code Generation Optimize for Math pyautogen supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the [retrievechat] option to use it. pyautogen pip install "pyautogen[retrievechat]" pip install "pyautogen[retrievechat]" RetrieveChat can handle various types of documents. By default, it can process
plain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',
'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.
If you install unstructured
(pip install "unstructured[all-docs]"), additional document types such as 'docx',
'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported. pip install "unstructured[all-docs]" You can find a list of all supported document types by using autogen.retrieve_utils.TEXT_FORMATS. autogen.retrieve_utils.TEXT_FORMATS Example notebooks: Automated Code Generation and Question Answering with Retrieval Augmented Agents Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent) Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents To use TeachableAgent, please install AutoGen with the [teachable] option. pip install "pyautogen[teachable]" pip install "pyautogen[teachable]" Example notebook:  Chatting with TeachableAgent We offered Multimodal Conversable Agent and LLaVA Agent. Please install with the [lmm] option to use it. pip install "pyautogen[lmm]" pip install "pyautogen[lmm]" Example notebooks: LLaVA Agent pyautogen<0.2 offers an experimental agent for math problem solving. Please install with the [mathchat] option to use it. pyautogen<0.2 pip install "pyautogen[mathchat]<0.2" pip install "pyautogen[mathchat]<0.2" Example notebooks: Using MathChat to Solve Math Problemsclass Agent() class Agent() (In preview) An abstract class for AI agent. An agent can communicate with other agents and perform actions.
Different agents can differ in what actions they perform in the receive method. receive def __init__(name: str) def __init__(name: str) Arguments: name @propertydef name() @propertydef name() Get the name of the agent. def send(message: Union[Dict, str],         recipient: "Agent",         request_reply: Optional[bool] = None) def send(message: Union[Dict, str],         recipient: "Agent",         request_reply: Optional[bool] = None) (Abstract method) Send a message to another agent. async def a_send(message: Union[Dict, str],                 recipient: "Agent",                 request_reply: Optional[bool] = None) async def a_send(message: Union[Dict, str],                 recipient: "Agent",                 request_reply: Optional[bool] = None) (Abstract async method) Send a message to another agent. def receive(message: Union[Dict, str],            sender: "Agent",            request_reply: Optional[bool] = None) def receive(message: Union[Dict, str],            sender: "Agent",            request_reply: Optional[bool] = None) (Abstract method) Receive a message from another agent. async def a_receive(message: Union[Dict, str],                    sender: "Agent",                    request_reply: Optional[bool] = None) async def a_receive(message: Union[Dict, str],                    sender: "Agent",                    request_reply: Optional[bool] = None) (Abstract async method) Receive a message from another agent. def reset() def reset() (Abstract method) Reset the agent. def generate_reply(messages: Optional[List[Dict]] = None,                   sender: Optional["Agent"] = None,                   **kwargs) -> Union[str, Dict, None] def generate_reply(messages: Optional[List[Dict]] = None,                   sender: Optional["Agent"] = None,                   **kwargs) -> Union[str, Dict, None] (Abstract method) Generate a reply based on the received messages. Arguments: messages sender Returns:   str or dict or None: the generated reply. If None, no reply is generated. async def a_generate_reply(messages: Optional[List[Dict]] = None,                           sender: Optional["Agent"] = None,                           **kwargs) -> Union[str, Dict, None] async def a_generate_reply(messages: Optional[List[Dict]] = None,                           sender: Optional["Agent"] = None,                           **kwargs) -> Union[str, Dict, None] (Abstract async method) Generate a reply based on the received messages. Arguments: messages sender Returns:   str or dict or None: the generated reply. If None, no reply is generated.class CompressibleAgent(ConversableAgent) class CompressibleAgent(ConversableAgent) (Experimental) CompressibleAgent agent. While this agent retains all the default functionalities of the AssistantAgent,
it also provides the added feature of compression when activated through the compress_config setting. AssistantAgent compress_config compress_config is set to False by default, making this agent equivalent to the AssistantAgent.
This agent does not work well in a GroupChat: The compressed messages will not be sent to all the agents in the group.
The default system message is the same as AssistantAgent.
human_input_mode is default to "NEVER"
and code_execution_config is default to False.
This agent doesn't execute code or function call by default. compress_config AssistantAgent human_input_mode code_execution_config def __init__(name: str,             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "NEVER",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict, bool]] = False,             llm_config: Optional[Union[Dict, bool]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             compress_config: Optional[Dict] = False) def __init__(name: str,             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "NEVER",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict, bool]] = False,             llm_config: Optional[Union[Dict, bool]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             compress_config: Optional[Dict] = False) Arguments: name system_message llm_config is_termination_msg max_consecutive_auto_reply compress_config "TERMINATE" trigger_count "COMPRESS" "CUSTOMIZED" **kwargs def generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] def generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] Adding to line 202:     if messages is not None and messages != self._oai_messages[sender]:        messages = self._oai_messages[sender]     if messages is not None and messages != self._oai_messages[sender]:        messages = self._oai_messages[sender] def on_oai_token_limit(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]] def on_oai_token_limit(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]] (Experimental) Compress previous messages when a threshold of tokens is reached. TODO: async compress
TODO: maintain a list for old oai messages (messages before compression) def compress_messages(        messages: Optional[List[Dict]] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None, List]] def compress_messages(        messages: Optional[List[Dict]] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None, List]] Compress a list of messages into one message. The first message (the initial prompt) will not be compressed.
The rest of the messages will be compressed into one message, the model is asked to distinguish the role of each message: USER, ASSISTANT, FUNCTION_CALL, FUNCTION_RETURN.
Check out the compress_sys_msg. TODO: model used in compression agent is different from assistant agent: For example, if original model used by is gpt-4; we start compressing at 70% of usage, 70% of 8092 = 5664; and we use gpt 3.5 here max_toke = 4096, it will raise error. choosinng model automatically?class RetrieveUserProxyAgent(UserProxyAgent) class RetrieveUserProxyAgent(UserProxyAgent) def __init__(name="RetrieveChatAgent",             human_input_mode: Optional[str] = "ALWAYS",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             retrieve_config: Optional[Dict] = None,             **kwargs) def __init__(name="RetrieveChatAgent",             human_input_mode: Optional[str] = "ALWAYS",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             retrieve_config: Optional[Dict] = None,             **kwargs) Arguments: name str - name of the agent. name human_input_mode str - whether to ask for human inputs every time a message is received.
Possible values are "ALWAYS", "TERMINATE", "NEVER".
(1) When "ALWAYS", the agent prompts for human input every time a message is received.
Under this mode, the conversation stops when the human input is "exit",
or when is_termination_msg is True and there is no human input.
(2) When "TERMINATE", the agent only prompts for human input only when a termination message is received or
the number of auto reply reaches the max_consecutive_auto_reply.
(3) When "NEVER", the agent will never prompt for human input. Under this mode, the conversation stops
when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True. human_input_mode is_termination_msg function - a function that takes a message in the form of a dictionary
and returns a boolean value indicating if this received message is a termination message.
The dict can contain the following keys: "content", "role", "name", "function_call". is_termination_msg retrieve_config dict or None - config for the retrieve agent.
To use default config, set to None. Otherwise, set to a dictionary with the following keys: retrieve_config default chromadb.Client() retrieve_docs autogen-docs gpt-4 max_tokens * 0.4 max_tokens * 0.8 multi_lines all-MiniLM-L6-v2 https://www.sbert.net/docs/pretrained_models.html all-mpnet-base-v2 embedding_model https://docs.trychroma.com/embeddings Update Context Update Context autogen.retrieve_utils.split_text_to_chunks autogen.retrieve_utils.TEXT_FORMATS docs_path **kwargs dict - other kwargs in UserProxyAgent. **kwargs Example of overriding retrieve_docs:
If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code. class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):    def query_vector_db(        self,        query_texts: List[str],        n_results: int = 10,        search_string: str = "",        **kwargs,    ) -> Dict[str, Union[List[str], List[List[str]]]]:        # define your own query function here        pass    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):        results = self.query_vector_db(            query_texts=[problem],            n_results=n_results,            search_string=search_string,            **kwargs,        )        self._results = results        print("doc_ids: ", results["ids"]) class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):    def query_vector_db(        self,        query_texts: List[str],        n_results: int = 10,        search_string: str = "",        **kwargs,    ) -> Dict[str, Union[List[str], List[List[str]]]]:        # define your own query function here        pass    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):        results = self.query_vector_db(            query_texts=[problem],            n_results=n_results,            search_string=search_string,            **kwargs,        )        self._results = results        print("doc_ids: ", results["ids"]) def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "") def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "") Retrieve docs based on the given problem and assign the results to the class property _results.
In case you want to customize the retrieval process, such as using a different vector db whose APIs are not
compatible with chromadb or filter results with metadata, you can override this function. Just keep the current
parameters and add your own parameters with default values, and keep the results in below type. _results Type of the results: Dict[str, List[List[Any]]], should have keys "ids" and "documents", "ids" for the ids of
the retrieved docs and "documents" for the contents of the retrieved docs. Any other keys are optional. Refer
to chromadb.api.types.QueryResult as an example.
ids: List[string]
documents: List[List[string]] chromadb.api.types.QueryResult Arguments: problem n_results search_string def generate_init_message(problem: str,                          n_results: int = 20,                          search_string: str = "") def generate_init_message(problem: str,                          n_results: int = 20,                          search_string: str = "") Generate an initial message with the given problem and prompt. Arguments: problem n_results search_string Returns: strclass ConversableAgent(Agent) class ConversableAgent(Agent) (In preview) A class for generic conversable agents which can be configured as assistant or user proxy. After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.
For example, AssistantAgent and UserProxyAgent are subclasses of this class,
configured with different default settings. To modify auto reply, override generate_reply method.
To disable/enable human response in every turn, set human_input_mode to "NEVER" or "ALWAYS".
To modify the way to get human input, override get_human_input method.
To modify the way to execute code blocks, single code block, or function call, override execute_code_blocks,
run_code, and execute_function methods respectively.
To customize the initial message when a conversation starts, override generate_init_message method. generate_reply human_input_mode get_human_input execute_code_blocks run_code execute_function generate_init_message maximum number of consecutive auto replies (subject to future change) def __init__(name: str,             system_message: Optional[Union[                 str, List]] = "You are a helpful AI Assistant.",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "TERMINATE",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = None,             llm_config: Optional[Union[Dict, Literal[False]]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             description: Optional[str] = None) def __init__(name: str,             system_message: Optional[Union[                 str, List]] = "You are a helpful AI Assistant.",             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "TERMINATE",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = None,             llm_config: Optional[Union[Dict, Literal[False]]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             description: Optional[str] = None) Arguments: name system_message is_termination_msg max_consecutive_auto_reply human_input_mode function_map code_execution_config llm_config default_auto_reply description def register_reply(trigger: Union[Type[Agent], str, Agent,                                  Callable[[Agent], bool], List],                   reply_func: Callable,                   position: int = 0,                   config: Optional[Any] = None,                   reset_config: Optional[Callable] = None) def register_reply(trigger: Union[Type[Agent], str, Agent,                                  Callable[[Agent], bool], List],                   reply_func: Callable,                   position: int = 0,                   config: Optional[Any] = None,                   reset_config: Optional[Callable] = None) Register a reply function. The reply function will be called when the trigger matches the sender.
The function registered later will be checked earlier by default.
To change the order, set the position to a positive integer. Arguments: trigger Note None sender=None reply_func def reply_func(    recipient: ConversableAgent,    messages: Optional[List[Dict]] = None,    sender: Optional[Agent] = None,    config: Optional[Any] = None,) -> Tuple[bool, Union[str, Dict, None]]: def reply_func(    recipient: ConversableAgent,    messages: Optional[List[Dict]] = None,    sender: Optional[Agent] = None,    config: Optional[Any] = None,) -> Tuple[bool, Union[str, Dict, None]]: position config reset_config def reset_config(config: Any) @propertydef system_message() -> Union[str, List] @propertydef system_message() -> Union[str, List] Return the system message. def update_system_message(system_message: Union[str, List]) def update_system_message(system_message: Union[str, List]) Update the system message. Arguments: system_message def update_max_consecutive_auto_reply(value: int,                                      sender: Optional[Agent] = None) def update_max_consecutive_auto_reply(value: int,                                      sender: Optional[Agent] = None) Update the maximum number of consecutive auto replies. Arguments: value sender def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int The maximum number of consecutive auto replies. @propertydef chat_messages() -> Dict[Agent, List[Dict]] @propertydef chat_messages() -> Dict[Agent, List[Dict]] A dictionary of conversations from agent to list of messages. def last_message(agent: Optional[Agent] = None) -> Optional[Dict] def last_message(agent: Optional[Agent] = None) -> Optional[Dict] The last message exchanged with the agent. Arguments: agent Returns:   The last message exchanged with the agent. @propertydef use_docker() -> Union[bool, str, None] @propertydef use_docker() -> Union[bool, str, None] Bool value of whether to use docker to execute the code,
or str value of the docker image name to use, or None when code execution is disabled. def send(message: Union[Dict, str],         recipient: Agent,         request_reply: Optional[bool] = None,         silent: Optional[bool] = False) def send(message: Union[Dict, str],         recipient: Agent,         request_reply: Optional[bool] = None,         silent: Optional[bool] = False) Send a message to another agent. Arguments: message {    "content": lambda context: context["use_tool_msg"],    "context": {        "use_tool_msg": "Use tool X if they are relevant."    }} {    "content": lambda context: context["use_tool_msg"],    "context": {        "use_tool_msg": "Use tool X if they are relevant."    }}   Next time, one agent can send a message B with a different "use_tool_msg".
Then the content of message A will be refreshed to the new "use_tool_msg".
So effectively, this provides a way for an agent to send a "link" and modify
the content of the "link" later. recipient request_reply silent Raises: ValueError async def a_send(message: Union[Dict, str],                 recipient: Agent,                 request_reply: Optional[bool] = None,                 silent: Optional[bool] = False) async def a_send(message: Union[Dict, str],                 recipient: Agent,                 request_reply: Optional[bool] = None,                 silent: Optional[bool] = False) (async) Send a message to another agent. Arguments: message {    "content": lambda context: context["use_tool_msg"],    "context": {        "use_tool_msg": "Use tool X if they are relevant."    }} {    "content": lambda context: context["use_tool_msg"],    "context": {        "use_tool_msg": "Use tool X if they are relevant."    }}   Next time, one agent can send a message B with a different "use_tool_msg".
Then the content of message A will be refreshed to the new "use_tool_msg".
So effectively, this provides a way for an agent to send a "link" and modify
the content of the "link" later. recipient request_reply silent Raises: ValueError def receive(message: Union[Dict, str],            sender: Agent,            request_reply: Optional[bool] = None,            silent: Optional[bool] = False) def receive(message: Union[Dict, str],            sender: Agent,            request_reply: Optional[bool] = None,            silent: Optional[bool] = False) Receive a message from another agent. Once a message is received, this function sends a reply to the sender or stop.
The reply can be generated automatically or entered manually by a human. Arguments: message sender request_reply self.reply_at_receive[sender] silent Raises: ValueError async def a_receive(message: Union[Dict, str],                    sender: Agent,                    request_reply: Optional[bool] = None,                    silent: Optional[bool] = False) async def a_receive(message: Union[Dict, str],                    sender: Agent,                    request_reply: Optional[bool] = None,                    silent: Optional[bool] = False) (async) Receive a message from another agent. Once a message is received, this function sends a reply to the sender or stop.
The reply can be generated automatically or entered manually by a human. Arguments: message sender request_reply self.reply_at_receive[sender] silent Raises: ValueError def initiate_chat(recipient: "ConversableAgent",                  clear_history: Optional[bool] = True,                  silent: Optional[bool] = False,                  **context) def initiate_chat(recipient: "ConversableAgent",                  clear_history: Optional[bool] = True,                  silent: Optional[bool] = False,                  **context) Initiate a chat with the recipient agent. Reset the consecutive auto reply counter.
If clear_history is True, the chat history with the recipient agent will be cleared.
generate_init_message is called to generate the initial message for the agent. clear_history generate_init_message Arguments: recipient clear_history silent **context generate_init_message async def a_initiate_chat(recipient: "ConversableAgent",                          clear_history: Optional[bool] = True,                          silent: Optional[bool] = False,                          **context) async def a_initiate_chat(recipient: "ConversableAgent",                          clear_history: Optional[bool] = True,                          silent: Optional[bool] = False,                          **context) (async) Initiate a chat with the recipient agent. Reset the consecutive auto reply counter.
If clear_history is True, the chat history with the recipient agent will be cleared.
generate_init_message is called to generate the initial message for the agent. clear_history generate_init_message Arguments: recipient clear_history silent **context generate_init_message def reset() def reset() Reset the agent. def stop_reply_at_receive(sender: Optional[Agent] = None) def stop_reply_at_receive(sender: Optional[Agent] = None) Reset the reply_at_receive of the sender. def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None) def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None) Reset the consecutive_auto_reply_counter of the sender. def clear_history(agent: Optional[Agent] = None) def clear_history(agent: Optional[Agent] = None) Clear the chat history of the agent. Arguments: agent def generate_oai_reply(    messages: Optional[List[Dict]] = None,    sender: Optional[Agent] = None,    config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]] def generate_oai_reply(    messages: Optional[List[Dict]] = None,    sender: Optional[Agent] = None,    config: Optional[OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, None]] Generate a reply using autogen.oai. async def a_generate_oai_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]] async def a_generate_oai_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]] Generate a reply using autogen.oai asynchronously. def generate_code_execution_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Union[Dict, Literal[False]]] = None) def generate_code_execution_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Union[Dict, Literal[False]]] = None) Generate a reply using code execution. def generate_function_call_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]] def generate_function_call_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]] Generate a reply using function call. async def a_generate_function_call_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]] async def a_generate_function_call_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[Dict, None]] Generate a reply using async function call. def check_termination_and_human_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, None]] def check_termination_and_human_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, None]] Check if the conversation should be terminated, and if human reply is provided. This method checks for conditions that require the conversation to be terminated, such as reaching
a maximum number of consecutive auto-replies or encountering a termination message. Additionally,
it prompts for and processes human input based on the configured human input mode, which can be
'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter
for the conversation and prints relevant messages based on the human input received. Arguments: Returns: async def a_check_termination_and_human_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, None]] async def a_check_termination_and_human_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        config: Optional[Any] = None) -> Tuple[bool, Union[str, None]] (async) Check if the conversation should be terminated, and if human reply is provided. This method checks for conditions that require the conversation to be terminated, such as reaching
a maximum number of consecutive auto-replies or encountering a termination message. Additionally,
it prompts for and processes human input based on the configured human input mode, which can be
'ALWAYS', 'NEVER', or 'TERMINATE'. The method also manages the consecutive auto-reply counter
for the conversation and prints relevant messages based on the human input received. Arguments: Returns: def generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] def generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] Reply based on the conversation history and the sender. Either messages or sender must be provided.
Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.
Use registered auto reply functions to generate replies.
By default, the following functions are checked in order: None messages sender None Arguments: messages default_reply sender exclude Returns:   str or dict or None: reply. None if no reply is generated. async def a_generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] async def a_generate_reply(        messages: Optional[List[Dict]] = None,        sender: Optional[Agent] = None,        exclude: Optional[List[Callable]] = None) -> Union[str, Dict, None] (async) Reply based on the conversation history and the sender. Either messages or sender must be provided.
Register a reply_func with None as one trigger for it to be activated when messages is non-empty and sender is None.
Use registered auto reply functions to generate replies.
By default, the following functions are checked in order: None messages sender None Arguments: messages default_reply sender exclude Returns:   str or dict or None: reply. None if no reply is generated. def get_human_input(prompt: str) -> str def get_human_input(prompt: str) -> str Get human input. Override this method to customize the way to get human input. Arguments: prompt Returns: str async def a_get_human_input(prompt: str) -> str async def a_get_human_input(prompt: str) -> str (Async) Get human input. Override this method to customize the way to get human input. Arguments: prompt Returns: str def run_code(code, **kwargs) def run_code(code, **kwargs) Run the code and return the result. Override this function to modify the way to run the code. Arguments: code **kwargs Returns:   A tuple of (exitcode, logs, image). exitcode logs image def execute_code_blocks(code_blocks) def execute_code_blocks(code_blocks) Execute the code blocks and return the result. def execute_function(func_call,                     verbose: bool = False) -> Tuple[bool, Dict[str, str]] def execute_function(func_call,                     verbose: bool = False) -> Tuple[bool, Dict[str, str]] Execute a function call and return the result. Override this function to modify the way to execute a function call. Arguments: func_call Returns:   A tuple of (is_exec_success, result_dict). is_exec_success result_dict async def a_execute_function(func_call) async def a_execute_function(func_call) Execute an async function call and return the result. Override this function to modify the way async functions are executed. Arguments: func_call Returns:   A tuple of (is_exec_success, result_dict). is_exec_success result_dict def generate_init_message(**context) -> Union[str, Dict] def generate_init_message(**context) -> Union[str, Dict] Generate the initial message for the agent. Override this function to customize the initial message based on user's request.
If not overriden, "message" needs to be provided in the context. Arguments: **context def register_function(function_map: Dict[str, Callable]) def register_function(function_map: Dict[str, Callable]) Register functions to the agent. Arguments: function_map def update_function_signature(func_sig: Union[str, Dict], is_remove: None) def update_function_signature(func_sig: Union[str, Dict], is_remove: None) update a function_signature in the LLM configuration for function_call. Arguments: func_sig is_remove def can_execute_function(name: str) -> bool def can_execute_function(name: str) -> bool Whether the agent can execute the function. @propertydef function_map() -> Dict[str, Callable] @propertydef function_map() -> Dict[str, Callable] Return the function map. def register_for_llm(*,                     name: Optional[str] = None,                     description: Optional[str] = None) -> Callable[[F], F] def register_for_llm(*,                     name: Optional[str] = None,                     description: Optional[str] = None) -> Callable[[F], F] Decorator factory for registering a function to be used by an agent. It's return value is used to decorate a function to be registered to the agent. The function uses type hints to
specify the arguments and return type. The function name is used as the default name for the function,
but a custom name can be provided. The function description is used to describe the function in the
agent's configuration. Arguments:   name (optional(str)): name of the function. If None, the function name will be used (default: None).
description (optional(str)): description of the function (default: None). It is mandatory
for the initial decorator, but the following ones can omit it. Returns:   The decorator for registering a function to be used by an agent. Examples: ```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description="This is a very useful function")def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14) -> str:     return a + str(b * c)``` ```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description="This is a very useful function")def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14) -> str:     return a + str(b * c)``` def register_for_execution(name: Optional[str] = None) -> Callable[[F], F] def register_for_execution(name: Optional[str] = None) -> Callable[[F], F] Decorator factory for registering a function to be executed by an agent. It's return value is used to decorate a function to be registered to the agent. Arguments:   name (optional(str)): name of the function. If None, the function name will be used (default: None). Returns:   The decorator for registering a function to be used by an agent. Examples: ```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description="This is a very useful function")def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14):     return a + str(b * c)``` ```@user_proxy.register_for_execution()@agent2.register_for_llm()@agent1.register_for_llm(description="This is a very useful function")def my_function(a: Annotated[str, "description of a parameter"] = "a", b: int, c=3.14):     return a + str(b * c)```class LLaVAAgent(MultimodalConversableAgent) class LLaVAAgent(MultimodalConversableAgent) def __init__(name: str,             system_message: Optional[Tuple[str,                                            List]] = DEFAULT_LLAVA_SYS_MSG,             *args,             **kwargs) def __init__(name: str,             system_message: Optional[Tuple[str,                                            List]] = DEFAULT_LLAVA_SYS_MSG,             *args,             **kwargs) Arguments: name system_message **kwargs def llava_call(prompt: str, llm_config: dict) -> str def llava_call(prompt: str, llm_config: dict) -> str Makes a call to the LLaVA service to generate text based on a given promptclass RetrieveAssistantAgent(AssistantAgent) class RetrieveAssistantAgent(AssistantAgent) (Experimental) Retrieve Assistant agent, designed to solve a task with LLM. RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.
The default system message is designed to solve a task with LLM,
including suggesting python code blocks and debugging.
human_input_mode is default to "NEVER"
and code_execution_config is default to False.
This agent doesn't execute code by default, and expects the user to execute the code. human_input_mode code_execution_configdef get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any def get_typed_annotation(annotation: Any, globalns: Dict[str, Any]) -> Any Get the type annotation of a parameter. Arguments: annotation globalns Returns:   The type annotation of the parameter def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature def get_typed_signature(call: Callable[..., Any]) -> inspect.Signature Get the signature of a function with type annotations. Arguments: call Returns:   The signature of the function with type annotations def get_typed_return_annotation(call: Callable[..., Any]) -> Any def get_typed_return_annotation(call: Callable[..., Any]) -> Any Get the return annotation of a function. Arguments: call Returns:   The return annotation of the function def get_param_annotations(    typed_signature: inspect.Signature) -> Dict[int, Union[Annotated[Type, str], Type]] def get_param_annotations(    typed_signature: inspect.Signature) -> Dict[int, Union[Annotated[Type, str], Type]] Get the type annotations of the parameters of a function Arguments: typed_signature Returns:   A dictionary of the type annotations of the parameters of the function class Parameters(BaseModel) class Parameters(BaseModel) Parameters of a function as defined by the OpenAI API class Function(BaseModel) class Function(BaseModel) A function as defined by the OpenAI API def get_parameter_json_schema(        k: str, v: Union[Annotated[Type, str], Type],        default_values: Dict[str, Any]) -> JsonSchemaValue def get_parameter_json_schema(        k: str, v: Union[Annotated[Type, str], Type],        default_values: Dict[str, Any]) -> JsonSchemaValue Get a JSON schema for a parameter as defined by the OpenAI API Arguments: k v default_values Returns:   A Pydanitc model for the parameter def get_required_params(typed_signature: inspect.Signature) -> List[str] def get_required_params(typed_signature: inspect.Signature) -> List[str] Get the required parameters of a function Arguments: signature Returns:   A list of the required parameters of the function def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any] def get_default_values(typed_signature: inspect.Signature) -> Dict[str, Any] Get default values of parameters of a function Arguments: signature Returns:   A dictionary of the default values of the parameters of the function def get_parameters(required: List[str],                   param_annotations: Dict[str, Union[Annotated[Type, str],                                                      Type]],                   default_values: Dict[str, Any]) -> Parameters def get_parameters(required: List[str],                   param_annotations: Dict[str, Union[Annotated[Type, str],                                                      Type]],                   default_values: Dict[str, Any]) -> Parameters Get the parameters of a function as defined by the OpenAI API Arguments: required hints Returns:   A Pydantic model for the parameters of the function def get_missing_annotations(typed_signature: inspect.Signature,                            required: List[str]) -> Tuple[Set[str], Set[str]] def get_missing_annotations(typed_signature: inspect.Signature,                            required: List[str]) -> Tuple[Set[str], Set[str]] Get the missing annotations of a function Ignores the parameters with default values as they are not required to be annotated, but logs a warning. Arguments: typed_signature required Returns:   A set of the missing annotations of the function def get_function_schema(f: Callable[..., Any],                        *,                        name: Optional[str] = None,                        description: str) -> Dict[str, Any] def get_function_schema(f: Callable[..., Any],                        *,                        name: Optional[str] = None,                        description: str) -> Dict[str, Any] Get a JSON schema for a function as defined by the OpenAI API Arguments: f name description Returns:   A JSON schema for the function Raises: TypeError Examples: ```def f(a: Annotated[str, "Parameter a"], b: int = 2, c: Annotated[float, "Parameter c"] = 0.1) -> None:    passget_function_schema(f, description="function f")#   {'type': 'function',#    'function': {'description': 'function f',#        'name': 'f',#        'parameters': {'type': 'object',#           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},#               'b': {'type': 'int', 'description': 'b'},#               'c': {'type': 'float', 'description': 'Parameter c'}},#           'required': ['a']}}}    ``` ```def f(a: Annotated[str, "Parameter a"], b: int = 2, c: Annotated[float, "Parameter c"] = 0.1) -> None:    passget_function_schema(f, description="function f")#   {'type': 'function',#    'function': {'description': 'function f',#        'name': 'f',#        'parameters': {'type': 'object',#           'properties': {'a': {'type': 'str', 'description': 'Parameter a'},#               'b': {'type': 'int', 'description': 'b'},#               'c': {'type': 'float', 'description': 'Parameter c'}},#           'required': ['a']}}}    ``` def get_load_param_if_needed_function(        t: Any) -> Optional[Callable[[T, Type], BaseModel]] def get_load_param_if_needed_function(        t: Any) -> Optional[Callable[[T, Type], BaseModel]] Get a function to load a parameter if it is a Pydantic model Arguments: t Returns:   A function to load the parameter if it is a Pydantic model, otherwise None def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any] def load_basemodels_if_needed(func: Callable[..., Any]) -> Callable[..., Any] A decorator to load the parameters of a function if they are Pydantic models Arguments: func Returns:   A function that loads the parameters before calling the original functionclass MathUserProxyAgent(UserProxyAgent) class MathUserProxyAgent(UserProxyAgent) (Experimental) A MathChat agent that can handle math problems. maximum number of consecutive auto replies (subject to future change) def __init__(name: Optional[str] = "MathChatAgent",             is_termination_msg: Optional[Callable[                 [Dict], bool]] = _is_termination_msg_mathchat,             human_input_mode: Optional[str] = "NEVER",             default_auto_reply: Optional[Union[str, Dict,                                                None]] = DEFAULT_REPLY,             max_invalid_q_per_step=3,             **kwargs) def __init__(name: Optional[str] = "MathChatAgent",             is_termination_msg: Optional[Callable[                 [Dict], bool]] = _is_termination_msg_mathchat,             human_input_mode: Optional[str] = "NEVER",             default_auto_reply: Optional[Union[str, Dict,                                                None]] = DEFAULT_REPLY,             max_invalid_q_per_step=3,             **kwargs) Arguments: name is_termination_msg human_input_mode default_auto_reply max_invalid_q_per_step **kwargs def generate_init_message(problem,                          prompt_type="default",                          customized_prompt=None) def generate_init_message(problem,                          prompt_type="default",                          customized_prompt=None) Generate a prompt for the assistant agent with the given problem and prompt. Arguments: problem prompt_type customized_prompt Returns: str def execute_one_python_code(pycode) def execute_one_python_code(pycode) Execute python code blocks. Previous python code will be saved and executed together with the new code.
the "print" function will also be added to the last line of the code if needed def execute_one_wolfram_query(query: str) def execute_one_wolfram_query(query: str) Run one wolfram query and return the output. Arguments: query Returns: output is_success def get_from_dict_or_env(data: Dict[str, Any],                         key: str,                         env_key: str,                         default: Optional[str] = None) -> str def get_from_dict_or_env(data: Dict[str, Any],                         key: str,                         env_key: str,                         default: Optional[str] = None) -> str Get a value from a dictionary or an environment variable. class WolframAlphaAPIWrapper(BaseModel) class WolframAlphaAPIWrapper(BaseModel) Wrapper for Wolfram Alpha. Docs for using: :meta private: class Config() class Config() Configuration for this pydantic object. @root_validator(skip_on_failure=True)def validate_environment(cls, values: Dict) -> Dict @root_validator(skip_on_failure=True)def validate_environment(cls, values: Dict) -> Dict Validate that api key and python package exists in environment. def run(query: str) -> Tuple[str, bool] def run(query: str) -> Tuple[str, bool] Run query through WolframAlpha and parse result.class UserProxyAgent(ConversableAgent) class UserProxyAgent(ConversableAgent) (In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents. UserProxyAgent is a subclass of ConversableAgent configured with human_input_mode to ALWAYS
and llm_config to False. By default, the agent will prompt for human input every time a message is received.
Code execution is enabled by default. LLM-based auto reply is disabled by default.
To modify auto reply, register a method with register_reply.
To modify the way to get human input, override get_human_input method.
To modify the way to execute code blocks, single code block, or function call, override execute_code_blocks,
run_code, and execute_function methods respectively.
To customize the initial message when a conversation starts, override generate_init_message method. human_input_mode llm_config register_reply get_human_input execute_code_blocks run_code execute_function generate_init_message def __init__(name: str,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "ALWAYS",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             llm_config: Optional[Union[Dict, Literal[False]]] = False,             system_message: Optional[Union[str, List]] = "",             description: Optional[str] = None) def __init__(name: str,             is_termination_msg: Optional[Callable[[Dict], bool]] = None,             max_consecutive_auto_reply: Optional[int] = None,             human_input_mode: Optional[str] = "ALWAYS",             function_map: Optional[Dict[str, Callable]] = None,             code_execution_config: Optional[Union[Dict,                                                   Literal[False]]] = None,             default_auto_reply: Optional[Union[str, Dict, None]] = "",             llm_config: Optional[Union[Dict, Literal[False]]] = False,             system_message: Optional[Union[str, List]] = "",             description: Optional[str] = None) Arguments: name is_termination_msg max_consecutive_auto_reply human_input_mode function_map code_execution_config default_auto_reply llm_config system_message descriptiondef content_str(content: Union[str, List, None]) -> str def content_str(content: Union[str, List, None]) -> str Converts content into a string format. content This function processes content that may be a string, a list of mixed text and image URLs, or None,
and converts it into a string. Text is directly appended to the result string, while image URLs are
represented by a placeholder image token. If the content is None, an empty string is returned. Arguments: Returns: str Notes: def infer_lang(code) def infer_lang(code) infer the language for the code.
TODO: make it robust. def extract_code(        text: Union[str, List],        pattern: str = CODE_BLOCK_PATTERN,        detect_single_line_code: bool = False) -> List[Tuple[str, str]] def extract_code(        text: Union[str, List],        pattern: str = CODE_BLOCK_PATTERN,        detect_single_line_code: bool = False) -> List[Tuple[str, str]] Extract code from a text. Arguments: text pattern detect_single_line_code Returns: list def generate_code(pattern: str = CODE_BLOCK_PATTERN,                  **config) -> Tuple[str, float] def generate_code(pattern: str = CODE_BLOCK_PATTERN,                  **config) -> Tuple[str, float] (openai<1) Generate code. Arguments: pattern config Returns: str float def improve_function(file_name, func_name, objective, **config) def improve_function(file_name, func_name, objective, **config) (openai<1) Improve the function to achieve the objective. def improve_code(files, objective, suggest_only=True, **config) def improve_code(files, objective, suggest_only=True, **config) (openai<1) Improve the code to achieve a given objective. Arguments: files objective suggest_only config Returns: str float def execute_code(code: Optional[str] = None,                 timeout: Optional[int] = None,                 filename: Optional[str] = None,                 work_dir: Optional[str] = None,                 use_docker: Optional[Union[List[str], str, bool]] = None,                 lang: Optional[str] = "python") -> Tuple[int, str, str] def execute_code(code: Optional[str] = None,                 timeout: Optional[int] = None,                 filename: Optional[str] = None,                 work_dir: Optional[str] = None,                 use_docker: Optional[Union[List[str], str, bool]] = None,                 lang: Optional[str] = "python") -> Tuple[int, str, str] Execute code in a docker container.
This function is not tested on MacOS. Arguments: code timeout filename code work_dir use_docker use_docker use_docker use_docker lang Returns: int str image def generate_assertions(definition: str, **config) -> Tuple[str, float] def generate_assertions(definition: str, **config) -> Tuple[str, float] (openai<1) Generate assertions for a function. Arguments: definition config Returns: str float def eval_function_completions(responses: List[str],                              definition: str,                              test: Optional[str] = None,                              entry_point: Optional[str] = None,                              assertions: Optional[Union[str, Callable[                                  [str], Tuple[str, float]]]] = None,                              timeout: Optional[float] = 3,                              use_docker: Optional[bool] = True) -> Dict def eval_function_completions(responses: List[str],                              definition: str,                              test: Optional[str] = None,                              entry_point: Optional[str] = None,                              assertions: Optional[Union[str, Callable[                                  [str], Tuple[str, float]]]] = None,                              timeout: Optional[float] = 3,                              use_docker: Optional[bool] = True) -> Dict (openai<1) Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test. Arguments: responses definition test entry_point assertions timeout Returns: dict class PassAssertionFilter() class PassAssertionFilter() def pass_assertions(context, response, **_) def pass_assertions(context, response, **_) (openai<1) Check if the response passes the assertions. def implement(    definition: str,    configs: Optional[List[Dict]] = None,    assertions: Optional[Union[str,                               Callable[[str],                                        Tuple[str,                                              float]]]] = generate_assertions) -> Tuple[str, float] def implement(    definition: str,    configs: Optional[List[Dict]] = None,    assertions: Optional[Union[str,                               Callable[[str],                                        Tuple[str,                                              float]]]] = generate_assertions) -> Tuple[str, float] (openai<1) Implement a function from a definition. Arguments: definition configs assertions Returns: str float intThis page lists libraries that have integrations with Autogen for LLM applications using multiple agents in alphabetical order. Including your own integration to this list is highly encouraged. Simply open a pull request with a few lines of text, see the dropdown below for more information.  MemGPT enables LLMs to manage their own memory and overcome limited context windows. You can use MemGPT to create perpetual chatbots that learn about you and modify their own personalities over time. You can connect MemGPT to your own local filesystems and databases, as well as connect MemGPT to your own tools and APIs. The MemGPT + AutoGen integration allows you to equip any AutoGen agent with MemGPT capabilities.class OpenAIWrapper() class OpenAIWrapper() A wrapper class for openai client. def __init__(*, config_list: List[Dict] = None, **base_config) def __init__(*, config_list: List[Dict] = None, **base_config) Arguments: config_list config_list=[    {        "model": "gpt-4",        "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),        "api_type": "azure",        "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),        "api_version": "2023-03-15-preview",    },    {        "model": "gpt-3.5-turbo",        "api_key": os.environ.get("OPENAI_API_KEY"),        "api_type": "open_ai",        "base_url": "https://api.openai.com/v1",    },    {        "model": "llama-7B",        "base_url": "http://127.0.0.1:8080",        "api_type": "open_ai",    }] config_list=[    {        "model": "gpt-4",        "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),        "api_type": "azure",        "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),        "api_version": "2023-03-15-preview",    },    {        "model": "gpt-3.5-turbo",        "api_key": os.environ.get("OPENAI_API_KEY"),        "api_type": "open_ai",        "base_url": "https://api.openai.com/v1",    },    {        "model": "llama-7B",        "base_url": "http://127.0.0.1:8080",        "api_type": "open_ai",    }] base_config def create(**config) def create(**config) Make a completion for a given config using openai's clients.
Besides the kwargs allowed in openai's client, we allow the following additional kwargs.
The config in each client will be overridden by the config. Arguments: prompt="Complete the following sentence: {prefix}, context={"prefix": "Today I feel"} cache_seed def yes_or_no_filter(context, response):    return context.get("yes_or_no_choice", False) is False or any(        text in ["Yes.", "No."] for text in client.extract_text_or_completion_object(response)    ) def yes_or_no_filter(context, response):    return context.get("yes_or_no_choice", False) is False or any(        text in ["Yes.", "No."] for text in client.extract_text_or_completion_object(response)    ) def print_usage_summary(        mode: Union[str, List[str]] = ["actual", "total"]) -> None def print_usage_summary(        mode: Union[str, List[str]] = ["actual", "total"]) -> None Print the usage summary. def clear_usage_summary() -> None def clear_usage_summary() -> None Clear the usage summary. def cost(response: Union[ChatCompletion, Completion]) -> float def cost(response: Union[ChatCompletion, Completion]) -> float Calculate the cost of the response. @classmethoddef extract_text_or_completion_object(    cls, response: ChatCompletion | Completion) -> Union[List[str], List[ChatCompletionMessage]] @classmethoddef extract_text_or_completion_object(    cls, response: ChatCompletion | Completion) -> Union[List[str], List[ChatCompletionMessage]] Extract the text or ChatCompletion objects from a completion or chat response. Arguments: response Returns:   A list of text, or a list of ChatCompletion objects if function_call/tool_calls are present.def token_left(input: Union[str, List, Dict],               model="gpt-3.5-turbo-0613") -> int def token_left(input: Union[str, List, Dict],               model="gpt-3.5-turbo-0613") -> int Count number of tokens left for an OpenAI model. Arguments: input model Returns: int def count_token(input: Union[str, List, Dict],                model: str = "gpt-3.5-turbo-0613") -> int def count_token(input: Union[str, List, Dict],                model: str = "gpt-3.5-turbo-0613") -> int Count number of tokens used by an OpenAI model. Arguments: input model Returns: int def num_tokens_from_functions(functions, model="gpt-3.5-turbo-0613") -> int def num_tokens_from_functions(functions, model="gpt-3.5-turbo-0613") -> int Return the number of tokens used by a list of functions. Arguments: functions model Returns: intautogen.OpenAIWrapper provides enhanced LLM inference for openai>=1.
autogen.Completion is a drop-in replacement of openai.Completion and openai.ChatCompletion for enhanced LLM inference using openai<1.
There are a number of benefits of using autogen to perform inference: performance tuning, API unification, caching, error handling, multi-config inference, result filtering, templating and so on. autogen.OpenAIWrapper openai>=1 autogen.Completion openai.Completion openai.ChatCompletion openai<1 autogen Find a list of examples in this page: Tune Inference Parameters Examples The cost of using foundation models for text generation is typically measured in terms of the number of tokens in the input and output combined. From the perspective of an application builder using foundation models, the use case is to maximize the utility of the generated text under an inference budget constraint (e.g., measured by the average dollar cost needed to solve a coding problem). This can be achieved by optimizing the hyperparameters of the inference,
which can significantly affect both the utility and the cost of the generated text. The tunable hyperparameters include: The cost and utility of text generation are intertwined with the joint effect of these hyperparameters.
There are also complex interactions among subsets of the hyperparameters. For example,
the temperature and top_p are not recommended to be altered from their default values together because they both control the randomness of the generated text, and changing both at the same time can result in conflicting effects; n and best_of are rarely tuned together because if the application can process multiple outputs, filtering on the server side causes unnecessary information loss; both n and max_tokens will affect the total number of tokens generated, which in turn will affect the cost of the request.
These interactions and trade-offs make it difficult to manually determine the optimal hyperparameter settings for a given text generation task. Do the choices matter? Check this blogpost to find example tuning results about gpt-3.5-turbo and gpt-4. With AutoGen, the tuning can be performed with the following information: Collect a diverse set of instances. They can be stored in an iterable of dicts. For example, each instance dict can contain "problem" as a key and the description str of a math problem as the value; and "solution" as a key and the solution str as the value. The evaluation function should take a list of responses, and other keyword arguments corresponding to the keys in each validation data instance as input, and output a dict of metrics. For example, def eval_math_responses(responses: List[str], solution: str, **args) -> Dict:    # select a response from the list of responses    answer = voted_answer(responses)    # check whether the answer is correct    return {"success": is_equivalent(answer, solution)} def eval_math_responses(responses: List[str], solution: str, **args) -> Dict:    # select a response from the list of responses    answer = voted_answer(responses)    # check whether the answer is correct    return {"success": is_equivalent(answer, solution)} autogen.code_utils and autogen.math_utils offer some example evaluation functions for code generation and math problem solving. autogen.code_utils autogen.math_utils The metric to optimize is usually an aggregated metric over all the tuning data instances. For example, users can specify "success" as the metric and "max" as the optimization mode. By default, the aggregation function is taking the average. Users can provide a customized aggregation function if needed. Users can specify the (optional) search range for each hyperparameter. flaml.tune.choice {problem} flaml.tune.randint flaml.tune.qrandint flaml.tune.lograndint flaml.qlograndint flaml.tune.uniform flaml.tune.loguniform flaml.tune.uniform One can specify an inference budget and an optimization budget.
The inference budget refers to the average inference cost per data instance.
The optimization budget refers to the total budget allowed in the tuning process. Both are measured by dollars and follow the price per 1000 tokens. Now, you can use autogen.Completion.tune for tuning. For example, autogen.Completion.tune import autogenconfig, analysis = autogen.Completion.tune(    data=tune_data,    metric="success",    mode="max",    eval_func=eval_func,    inference_budget=0.05,    optimization_budget=3,    num_samples=-1,) import autogenconfig, analysis = autogen.Completion.tune(    data=tune_data,    metric="success",    mode="max",    eval_func=eval_func,    inference_budget=0.05,    optimization_budget=3,    num_samples=-1,) num_samples is the number of configurations to sample. -1 means unlimited (until optimization budget is exhausted).
The returned config contains the optimized configuration and analysis contains an ExperimentAnalysis object for all the tried configurations and results. num_samples config analysis The tuned config can be used to perform inference. autogen.OpenAIWrapper.create() can be used to create completions for both chat and non-chat models, and both OpenAI API and Azure OpenAI API. autogen.OpenAIWrapper.create() from autogen import OpenAIWrapper# OpenAI endpointclient = OpenAIWrapper()# ChatCompletionresponse = client.create(messages=[{"role": "user", "content": "2+2="}], model="gpt-3.5-turbo")# extract the response textprint(client.extract_text_or_completion_object(response))# get cost of this completionprint(response.cost)# Azure OpenAI endpointclient = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type="azure")# Completionresponse = client.create(prompt="2+2=", model="gpt-3.5-turbo-instruct")# extract the response textprint(client.extract_text_or_completion_object(response)) from autogen import OpenAIWrapper# OpenAI endpointclient = OpenAIWrapper()# ChatCompletionresponse = client.create(messages=[{"role": "user", "content": "2+2="}], model="gpt-3.5-turbo")# extract the response textprint(client.extract_text_or_completion_object(response))# get cost of this completionprint(response.cost)# Azure OpenAI endpointclient = OpenAIWrapper(api_key=..., base_url=..., api_version=..., api_type="azure")# Completionresponse = client.create(prompt="2+2=", model="gpt-3.5-turbo-instruct")# extract the response textprint(client.extract_text_or_completion_object(response)) For local LLMs, one can spin up an endpoint using a package like FastChat, and then use the same API to send a request. See here for examples on how to make inference with local LLMs. The OpenAIWrapper from autogen tracks token counts and costs of your API calls. Use the create() method to initiate requests and print_usage_summary() to retrieve a detailed usage report, including total cost and token usage for both cached and actual requests. OpenAIWrapper autogen create() print_usage_summary() mode=["actual", "total"] mode='actual' mode='total' Reset your session's usage data with clear_usage_summary() when needed. View Notebook clear_usage_summary() Example usage: from autogen import OpenAIWrapperclient = OpenAIWrapper()client.create(messages=[{"role": "user", "content": "Python learning tips."}], model="gpt-3.5-turbo")client.print_usage_summary()  # Display usageclient.clear_usage_summary()  # Reset usage data from autogen import OpenAIWrapperclient = OpenAIWrapper()client.create(messages=[{"role": "user", "content": "Python learning tips."}], model="gpt-3.5-turbo")client.print_usage_summary()  # Display usageclient.clear_usage_summary()  # Reset usage data Sample output: Usage summary excluding cached usage:Total cost: 0.00015* Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83Usage summary including cached usage:Total cost: 0.00027* Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150 Usage summary excluding cached usage:Total cost: 0.00015* Model 'gpt-3.5-turbo': cost: 0.00015, prompt_tokens: 25, completion_tokens: 58, total_tokens: 83Usage summary including cached usage:Total cost: 0.00027* Model 'gpt-3.5-turbo': cost: 0.00027, prompt_tokens: 50, completion_tokens: 100, total_tokens: 150 API call results are cached locally and reused when the same request is issued. This is useful when repeating or continuing experiments for reproducibility and cost saving. It still allows controlled randomness by setting the "cache_seed" specified in OpenAIWrapper.create() or the constructor of OpenAIWrapper. OpenAIWrapper.create() OpenAIWrapper client = OpenAIWrapper(cache_seed=...)client.create(...) client = OpenAIWrapper(cache_seed=...)client.create(...) client = OpenAIWrapper()client.create(cache_seed=..., ...) client = OpenAIWrapper()client.create(cache_seed=..., ...) Caching is enabled by default with cache_seed 41. To disable it please set cache_seed to None. cache_seed NOTE. openai v1.1 introduces a new param seed. The difference between autogen's cache_seed and openai's seed is that: seed cache_seed seed seed seed cache_seed One can pass a list of configurations of different models/endpoints to mitigate the rate limits and other runtime error. For example, client = OpenAIWrapper(    config_list=[        {            "model": "gpt-4",            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),            "api_type": "azure",            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),            "api_version": "2023-08-01-preview",        },        {            "model": "gpt-3.5-turbo",            "api_key": os.environ.get("OPENAI_API_KEY"),            "base_url": "https://api.openai.com/v1",        },        {            "model": "llama2-chat-7B",            "base_url": "http://127.0.0.1:8080",        }    ],) client = OpenAIWrapper(    config_list=[        {            "model": "gpt-4",            "api_key": os.environ.get("AZURE_OPENAI_API_KEY"),            "api_type": "azure",            "base_url": os.environ.get("AZURE_OPENAI_API_BASE"),            "api_version": "2023-08-01-preview",        },        {            "model": "gpt-3.5-turbo",            "api_key": os.environ.get("OPENAI_API_KEY"),            "base_url": "https://api.openai.com/v1",        },        {            "model": "llama2-chat-7B",            "base_url": "http://127.0.0.1:8080",        }    ],) client.create() will try querying Azure OpenAI gpt-4, OpenAI gpt-3.5-turbo, and a locally hosted llama2-chat-7B one by one,
until a valid result is returned. This can speed up the development process where the rate limit is a bottleneck. An error will be raised if the last choice fails. So make sure the last choice in the list has the best availability. client.create() For convenience, we provide a number of utility functions to load config lists. get_config_list config_list_openai_aoai config_list_from_json config_list_from_models config_list_from_dotenv .env We suggest that you take a look at this notebook for full code examples of the different methods to configure your model endpoints. Another type of error is that the returned response does not satisfy a requirement. For example, if the response is required to be a valid json string, one would like to filter the responses that are not. This can be achieved by providing a list of configurations and a filter function. For example, def valid_json_filter(response, **_):    for text in OpenAIWrapper.extract_text_or_completion_object(response):        try:            json.loads(text)            return True        except ValueError:            pass    return Falseclient = OpenAIWrapper(    config_list=[{"model": "text-ada-001"}, {"model": "gpt-3.5-turbo-instruct"}, {"model": "text-davinci-003"}],)response = client.create(    prompt="How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.",    filter_func=valid_json_filter,) def valid_json_filter(response, **_):    for text in OpenAIWrapper.extract_text_or_completion_object(response):        try:            json.loads(text)            return True        except ValueError:            pass    return Falseclient = OpenAIWrapper(    config_list=[{"model": "text-ada-001"}, {"model": "gpt-3.5-turbo-instruct"}, {"model": "text-davinci-003"}],)response = client.create(    prompt="How to construct a json request to Bing API to search for 'latest AI news'? Return the JSON request.",    filter_func=valid_json_filter,) The example above will try to use text-ada-001, gpt-3.5-turbo-instruct, and text-davinci-003 iteratively, until a valid json string is returned or the last config is used. One can also repeat the same model in the list for multiple times (with different seeds) to try one model multiple times for increasing the robustness of the final response. Advanced use case: Check this blogpost to find how to improve GPT-4's coding performance from 68% to 90% while reducing the inference cost. If the provided prompt or message is a template, it will be automatically materialized with a given context. For example, response = client.create(    context={"problem": "How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?"},    prompt="{problem} Solve the problem carefully.",    allow_format_str_template=True,    **config) response = client.create(    context={"problem": "How many positive integers, not exceeding 100, are multiples of 2 or 3 but not 4?"},    prompt="{problem} Solve the problem carefully.",    allow_format_str_template=True,    **config) A template is either a format str, like the example above, or a function which produces a str from several input fields, like the example below. def content(turn, context):    return "\n".join(        [            context[f"user_message_{turn}"],            context[f"external_info_{turn}"]        ]    )messages = [    {        "role": "system",        "content": "You are a teaching assistant of math.",    },    {        "role": "user",        "content": partial(content, turn=0),    },]context = {    "user_message_0": "Could you explain the solution to Problem 1?",    "external_info_0": "Problem 1: ...",}response = client.create(context=context, messages=messages, **config)messages.append(    {        "role": "assistant",        "content": client.extract_text(response)[0]    })messages.append(    {        "role": "user",        "content": partial(content, turn=1),    },)context.append(    {        "user_message_1": "Why can't we apply Theorem 1 to Equation (2)?",        "external_info_1": "Theorem 1: ...",    })response = client.create(context=context, messages=messages, **config) def content(turn, context):    return "\n".join(        [            context[f"user_message_{turn}"],            context[f"external_info_{turn}"]        ]    )messages = [    {        "role": "system",        "content": "You are a teaching assistant of math.",    },    {        "role": "user",        "content": partial(content, turn=0),    },]context = {    "user_message_0": "Could you explain the solution to Problem 1?",    "external_info_0": "Problem 1: ...",}response = client.create(context=context, messages=messages, **config)messages.append(    {        "role": "assistant",        "content": client.extract_text(response)[0]    })messages.append(    {        "role": "user",        "content": partial(content, turn=1),    },)context.append(    {        "user_message_1": "Why can't we apply Theorem 1 to Equation (2)?",        "external_info_1": "Theorem 1: ...",    })response = client.create(context=context, messages=messages, **config) When debugging or diagnosing an LLM-based system, it is often convenient to log the API calls and analyze them. autogen.Completion and autogen.ChatCompletion offer an easy way to collect the API call histories. For example, to log the chat histories, simply run: autogen.Completion autogen.ChatCompletion autogen.ChatCompletion.start_logging() autogen.ChatCompletion.start_logging() The API calls made after this will be automatically logged. They can be retrieved at any time by: autogen.ChatCompletion.logged_history autogen.ChatCompletion.logged_history There is a function that can be used to print usage summary (total cost, and token count usage from each model): autogen.ChatCompletion.print_usage_summary() autogen.ChatCompletion.print_usage_summary() To stop logging, use autogen.ChatCompletion.stop_logging() autogen.ChatCompletion.stop_logging() If one would like to append the history to an existing dict, pass the dict like: autogen.ChatCompletion.start_logging(history_dict=existing_history_dict) autogen.ChatCompletion.start_logging(history_dict=existing_history_dict) By default, the counter of API calls will be reset at start_logging(). If no reset is desired, set reset_counter=False. start_logging() reset_counter=False There are two types of logging formats: compact logging and individual API call logging. The default format is compact.
Set compact=False in start_logging() to switch. compact=False start_logging() {    """    [        {            'role': 'system',            'content': system_message,        },        {            'role': 'user',            'content': user_message_1,        },        {            'role': 'assistant',            'content': assistant_message_1,        },        {            'role': 'user',            'content': user_message_2,        },        {            'role': 'assistant',            'content': assistant_message_2,        },    ]""": {        "created_at": [0, 1],        "cost": [0.1, 0.2],    }} {    """    [        {            'role': 'system',            'content': system_message,        },        {            'role': 'user',            'content': user_message_1,        },        {            'role': 'assistant',            'content': assistant_message_1,        },        {            'role': 'user',            'content': user_message_2,        },        {            'role': 'assistant',            'content': assistant_message_2,        },    ]""": {        "created_at": [0, 1],        "cost": [0.1, 0.2],    }} {    0: {        "request": {            "messages": [                {                    "role": "system",                    "content": system_message,                },                {                    "role": "user",                    "content": user_message_1,                }            ],            ... # other parameters in the request        },        "response": {            "choices": [                "messages": {                    "role": "assistant",                    "content": assistant_message_1,                },            ],            ... # other fields in the response        }    },    1: {        "request": {            "messages": [                {                    "role": "system",                    "content": system_message,                },                {                    "role": "user",                    "content": user_message_1,                },                {                    "role": "assistant",                    "content": assistant_message_1,                },                {                    "role": "user",                    "content": user_message_2,                },            ],            ... # other parameters in the request        },        "response": {            "choices": [                "messages": {                    "role": "assistant",                    "content": assistant_message_2,                },            ],            ... # other fields in the response        }    },} {    0: {        "request": {            "messages": [                {                    "role": "system",                    "content": system_message,                },                {                    "role": "user",                    "content": user_message_1,                }            ],            ... # other parameters in the request        },        "response": {            "choices": [                "messages": {                    "role": "assistant",                    "content": assistant_message_1,                },            ],            ... # other fields in the response        }    },    1: {        "request": {            "messages": [                {                    "role": "system",                    "content": system_message,                },                {                    "role": "user",                    "content": user_message_1,                },                {                    "role": "assistant",                    "content": assistant_message_1,                },                {                    "role": "user",                    "content": user_message_2,                },            ],            ... # other parameters in the request        },        "response": {            "choices": [                "messages": {                    "role": "assistant",                    "content": assistant_message_2,                },            ],            ... # other fields in the response        }    },} Total cost: <cost>Token count summary for model <model>: prompt_tokens: <count 1>, completion_tokens: <count 2>, total_tokens: <count 3> Total cost: <cost>Token count summary for model <model>: prompt_tokens: <count 1>, completion_tokens: <count 2>, total_tokens: <count 3> It can be seen that the individual API call history contains redundant information of the conversation. For a long conversation the degree of redundancy is high.
The compact history is more efficient and the individual API call history contains more details.These formats will be parsed by the 'unstructured' library, if installed. def split_text_to_chunks(text: str,                         max_tokens: int = 4000,                         chunk_mode: str = "multi_lines",                         must_break_at_empty_line: bool = True,                         overlap: int = 10) def split_text_to_chunks(text: str,                         max_tokens: int = 4000,                         chunk_mode: str = "multi_lines",                         must_break_at_empty_line: bool = True,                         overlap: int = 10) Split a long text into chunks of max_tokens. def extract_text_from_pdf(file: str) -> str def extract_text_from_pdf(file: str) -> str Extract text from PDF files def split_files_to_chunks(files: list,                          max_tokens: int = 4000,                          chunk_mode: str = "multi_lines",                          must_break_at_empty_line: bool = True,                          custom_text_split_function: Callable = None) def split_files_to_chunks(files: list,                          max_tokens: int = 4000,                          chunk_mode: str = "multi_lines",                          must_break_at_empty_line: bool = True,                          custom_text_split_function: Callable = None) Split a list of files into chunks of max_tokens. def get_files_from_dir(dir_path: Union[str, List[str]],                       types: list = TEXT_FORMATS,                       recursive: bool = True) def get_files_from_dir(dir_path: Union[str, List[str]],                       types: list = TEXT_FORMATS,                       recursive: bool = True) Return a list of all the files in a given directory, a url, a file path or a list of them. def get_file_from_url(url: str, save_path: str = None) def get_file_from_url(url: str, save_path: str = None) Download a file from a URL. def is_url(string: str) def is_url(string: str) Return True if the string is a valid URL. def create_vector_db_from_dir(dir_path: Union[str, List[str]],                              max_tokens: int = 4000,                              client: API = None,                              db_path: str = "/tmp/chromadb.db",                              collection_name: str = "all-my-documents",                              get_or_create: bool = False,                              chunk_mode: str = "multi_lines",                              must_break_at_empty_line: bool = True,                              embedding_model: str = "all-MiniLM-L6-v2",                              embedding_function: Callable = None,                              custom_text_split_function: Callable = None,                              custom_text_types: List[str] = TEXT_FORMATS,                              recursive: bool = True) -> API def create_vector_db_from_dir(dir_path: Union[str, List[str]],                              max_tokens: int = 4000,                              client: API = None,                              db_path: str = "/tmp/chromadb.db",                              collection_name: str = "all-my-documents",                              get_or_create: bool = False,                              chunk_mode: str = "multi_lines",                              must_break_at_empty_line: bool = True,                              embedding_model: str = "all-MiniLM-L6-v2",                              embedding_function: Callable = None,                              custom_text_split_function: Callable = None,                              custom_text_types: List[str] = TEXT_FORMATS,                              recursive: bool = True) -> API Create a vector db from all the files in a given directory, the directory can also be a single file or a url to
a single file. We support chromadb compatible APIs to create the vector db, this function is not required if
you prepared your own vector db. Arguments: dir_path max_tokens client db_path collection_name get_or_create chunk_mode must_break_at_empty_line embedding_model embedding_function embedding_model https://docs.trychroma.com/embeddings custom_text_split_function autogen.retrieve_utils.split_text_to_chunks custom_text_types recursive Returns: API def query_vector_db(query_texts: List[str],                    n_results: int = 10,                    client: API = None,                    db_path: str = "/tmp/chromadb.db",                    collection_name: str = "all-my-documents",                    search_string: str = "",                    embedding_model: str = "all-MiniLM-L6-v2",                    embedding_function: Callable = None) -> QueryResult def query_vector_db(query_texts: List[str],                    n_results: int = 10,                    client: API = None,                    db_path: str = "/tmp/chromadb.db",                    collection_name: str = "all-my-documents",                    search_string: str = "",                    embedding_model: str = "all-MiniLM-L6-v2",                    embedding_function: Callable = None) -> QueryResult Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db
and query function. Arguments: query_texts n_results client db_path collection_name search_string embedding_model embedding_function embedding_model https://docs.trychroma.com/embeddings Returns: QueryResult ids embeddings documents metadatas distancesclass AgentBuilder() class AgentBuilder() AgentBuilder can help user build an automatic task solving process powered by multi-agent system.
Specifically, our building pipeline includes initialize and build.
In build(), we prompt a gpt-4 model to create multiple participant agents, and specify whether
this task need programming to solve.
User can save the built agents' config by calling save(), and load the saved configs by load(), which can skip the
building process. maximum number of agents build manager can create. def __init__(config_path: Optional[str] = "OAI_CONFIG_LIST",             builder_model: Optional[str] = "gpt-4",             agent_model: Optional[str] = "gpt-4",             host: Optional[str] = "localhost",             endpoint_building_timeout: Optional[int] = 600) def __init__(config_path: Optional[str] = "OAI_CONFIG_LIST",             builder_model: Optional[str] = "gpt-4",             agent_model: Optional[str] = "gpt-4",             host: Optional[str] = "localhost",             endpoint_building_timeout: Optional[int] = 600) Arguments: config_path builder_model agent_model host endpoint_building_timeout def clear_agent(agent_name: str, recycle_endpoint: Optional[bool] = True) def clear_agent(agent_name: str, recycle_endpoint: Optional[bool] = True) Clear a specific agent by name. Arguments: agent_name recycle_endpoint def clear_all_agents(recycle_endpoint: Optional[bool] = True) def clear_all_agents(recycle_endpoint: Optional[bool] = True) Clear all cached agents. def build(building_task: Optional[str] = None,          default_llm_config: Optional[Dict] = None,          coding: Optional[bool] = None,          cached_configs: Optional[Dict] = None,          use_oai_assistant: Optional[bool] = False,          code_execution_config: Optional[Dict] = None,          **kwargs) def build(building_task: Optional[str] = None,          default_llm_config: Optional[Dict] = None,          coding: Optional[bool] = None,          cached_configs: Optional[Dict] = None,          use_oai_assistant: Optional[bool] = False,          code_execution_config: Optional[Dict] = None,          **kwargs) Auto build agents based on the building task. Arguments: building_task default_llm_config coding cached_configs use_oai_assistant code_execution_config def save(filepath: Optional[str] = None) -> str def save(filepath: Optional[str] = None) -> str Save building configs. If the filepath is not specific, this function will create a filename by encrypt the
buildingtask string by md5 with "save_config" prefix, and save config to the local path. Arguments: filepath Returns: filepath def load(filepath: Optional[str] = None,         config_json: Optional[str] = None,         **kwargs) def load(filepath: Optional[str] = None,         config_json: Optional[str] = None,         **kwargs) Load building configs and call the build function to complete building without calling online LLMs' api. Arguments: filepath config_jsondef solve_problem(problem: str, **config) -> str def solve_problem(problem: str, **config) -> str (openai<1) Solve the math problem. Arguments: problem config Returns: str def remove_boxed(string: str) -> Optional[str] def remove_boxed(string: str) -> Optional[str] Source: https://github.com/hendrycks/math
Extract the text within a \boxed{...} environment. Example: remove_boxed("\boxed{\frac{2}{3}}")   \frac{2}{3} def last_boxed_only_string(string: str) -> Optional[str] def last_boxed_only_string(string: str) -> Optional[str] Source: https://github.com/hendrycks/math
Extract the last \boxed{...} or \fbox{...} element from a string. def is_equiv(str1: Optional[str], str2: Optional[str]) -> float def is_equiv(str1: Optional[str], str2: Optional[str]) -> float Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in def is_equiv_chain_of_thought(str1: str, str2: str) -> float def is_equiv_chain_of_thought(str1: str, str2: str) -> float Strips the solution first before calling is_equiv. is_equiv def eval_math_responses(responses, solution=None, **args) def eval_math_responses(responses, solution=None, **args) Select a response for a math problem using voting, and check if the response is correct if the solution is provided. Arguments: responses solution Returns: dictclass MultimodalConversableAgent(ConversableAgent) class MultimodalConversableAgent(ConversableAgent) def __init__(name: str,             system_message: Optional[Union[str, List]] = DEFAULT_LMM_SYS_MSG,             is_termination_msg: str = None,             *args,             **kwargs) def __init__(name: str,             system_message: Optional[Union[str, List]] = DEFAULT_LMM_SYS_MSG,             is_termination_msg: str = None,             *args,             **kwargs) Arguments: name system_message **kwargs def update_system_message(system_message: Union[Dict, List, str]) def update_system_message(system_message: Union[Dict, List, str]) Update the system message. Arguments: system_messageclass TeachableAgent(ConversableAgent) class TeachableAgent(ConversableAgent) (Experimental) Teachable Agent, a subclass of ConversableAgent using a vector database to remember user teachings.
In this class, the term 'user' refers to any caller (human or not) sending messages to this agent.
Not yet tested in the group-chat setting. def __init__(        name="teachableagent",        system_message:    Optional[        str] = "You are a helpful AI assistant that remembers user teachings from prior chats.",        human_input_mode: Optional[str] = "NEVER",        llm_config: Optional[Union[Dict, bool]] = None,        analyzer_llm_config: Optional[Union[Dict, bool]] = None,        teach_config: Optional[Dict] = None,        **kwargs) def __init__(        name="teachableagent",        system_message:    Optional[        str] = "You are a helpful AI assistant that remembers user teachings from prior chats.",        human_input_mode: Optional[str] = "NEVER",        llm_config: Optional[Union[Dict, bool]] = None,        analyzer_llm_config: Optional[Union[Dict, bool]] = None,        teach_config: Optional[Dict] = None,        **kwargs) Arguments: name system_message human_input_mode llm_config analyzer_llm_config teach_config **kwargs def close_db() def close_db() Cleanly closes the memo store. def prepopulate_db() def prepopulate_db() Adds a few arbitrary memos to the DB. def learn_from_user_feedback() def learn_from_user_feedback() Reviews the user comments from the last chat, and decides what teachings to store as memos. def consider_memo_storage(comment) def consider_memo_storage(comment) Decides whether to store something from one user comment in the DB. def consider_memo_retrieval(comment) def consider_memo_retrieval(comment) Decides whether to retrieve memos from the DB, and add them to the chat context. def retrieve_relevant_memos(input_text) def retrieve_relevant_memos(input_text) Returns semantically related memos from the DB. def concatenate_memo_texts(memo_list) def concatenate_memo_texts(memo_list) Concatenates the memo texts into a single string for inclusion in the chat context. def analyze(text_to_analyze, analysis_instructions) def analyze(text_to_analyze, analysis_instructions) Asks TextAnalyzerAgent to analyze the given text according to specific instructions. class MemoStore() class MemoStore() (Experimental)
Provides memory storage and retrieval for a TeachableAgent, using a vector database.
Each DB entry (called a memo) is a pair of strings: an input text and an output text.
The input text might be a question, or a task to perform.
The output text might be an answer to the question, or advice on how to perform the task.
Vector embeddings are currently supplied by Chroma's default Sentence Transformers. def __init__(verbosity, reset, path_to_db_dir) def __init__(verbosity, reset, path_to_db_dir) Arguments: def list_memos() def list_memos() Prints the contents of MemoStore. def close() def close() Saves self.uid_text_dict to disk. def reset_db() def reset_db() Forces immediate deletion of the DB's contents, in memory and on disk. def add_input_output_pair(input_text, output_text) def add_input_output_pair(input_text, output_text) Adds an input-output pair to the vector DB. def get_nearest_memo(query_text) def get_nearest_memo(query_text) Retrieves the nearest memo to the given query text. def get_related_memos(query_text, n_results, threshold) def get_related_memos(query_text, n_results, threshold) Retrieves memos that are related to the given query text within the specified distance threshold. def prepopulate() def prepopulate() Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial.This page contains a list of demos that use AutoGen in various applications from the community. Contribution guide:
Built something interesting with AutoGen? Submit a PR to add it to the list! See the Contribution Guide below for more details. To contribute, please open a PR that adds an entry to the data/gallery.json file in the src directory. The entry should be an object with the following properties: data/gallery.json src {    "title": "AutoGen Playground",    "link": "https://huggingface.co/spaces/thinkall/AutoGen_Playground",    "description": "A space to explore the capabilities of AutoGen.",    "image": "default.png",    "tags": ["ui"]  } {    "title": "AutoGen Playground",    "link": "https://huggingface.co/spaces/thinkall/AutoGen_Playground",    "description": "A space to explore the capabilities of AutoGen.",    "image": "default.png",    "tags": ["ui"]  } The image property should be the name of a file in the static/img/gallery directory.
The tags property should be an array of strings that describe the demo. We recommend using no more than two tags for clarity.
Here are the meanings of several tags for reference: image static/img/gallery tags if the existing ones do not precisely portray your own demos, new tags are also encouraged to add.